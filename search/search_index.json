{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"PyTorch Image Models (timm)","text":"<p>Welcome to timm Documentation</p> <p>PyTorch Image Models (<code>timm</code>) is a collection of image models, layers, utilities, optimizers, schedulers, data-loaders / augmentations, and reference training / validation scripts that aim to pull together a wide variety of SOTA models with ability to reproduce ImageNet training results.</p>"},{"location":"#introduction","title":"Introduction","text":"<p>The work of many others is present here. All source material is acknowledged via links to github, arxiv papers, etc in the README, documentation, and code docstrings.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>1000+ Pre-trained Models: Wide variety of state-of-the-art architectures</li> <li>Consistent API: All models share common interfaces for feature extraction and classifier access</li> <li>Multi-scale Feature Extraction: Easy feature pyramid extraction from any model</li> <li>Production Ready: High-performance training and inference scripts</li> <li>Extensive Augmentations: Modern augmentation techniques including RandAugment, AutoAugment, CutMix, and more</li> <li>Modern Optimizers: Including LION, AdamW, LAMB, LARS, and many more</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>\ud83d\udd25 Complete Benchmark Table - All 150+ models in one sortable table</li> <li>Model Metrics - Browse performance metrics by year</li> <li>Getting Started - Installation and basic usage</li> <li>Official Documentation</li> <li>GitHub Repository</li> <li>Papers with Code</li> </ul>"},{"location":"#whats-new","title":"What's New","text":""},{"location":"#december-2025","title":"December 2025","text":"<ul> <li>Lightweight task abstraction added</li> <li>Logits and feature distillation support added to train script</li> <li>Removed old APEX AMP support</li> </ul>"},{"location":"#november-2025","title":"November 2025","text":"<ul> <li>Fixed LayerScale init bug</li> <li>EfficientNet-X and EfficientNet-H B5 model weights added</li> <li>Muon optimizer implementation added</li> </ul>"},{"location":"#october-2025","title":"October 2025","text":"<ul> <li>DINOv3 ConvNeXt and ViT models added</li> <li>MobileCLIP-2 vision encoders added</li> <li>MetaCLIP-2 Worldwide ViT encoder weights added</li> <li>SigLIP-2 NaFlex ViT encoder weights added</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install timm\n</code></pre> <p>For development installation:</p> <pre><code>git clone https://github.com/GeorgePearse/pytorch-image-models\ncd pytorch-image-models\npip install -e .\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>import timm\nimport torch\n\n# List all available models\nmodel_names = timm.list_models()\n\n# Create a model with pretrained weights\nmodel = timm.create_model('resnet50', pretrained=True)\nmodel.eval()\n\n# Prepare an image (3x224x224)\nx = torch.randn(1, 3, 224, 224)\n\n# Get predictions\noutput = model(x)\n</code></pre>"},{"location":"#citation","title":"Citation","text":"<pre><code>@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/rwightman/pytorch-image-models}}\n}\n</code></pre>"},{"location":"#license","title":"License","text":"<p>The code is licensed Apache 2.0. See Licenses for details on pretrained weights.</p>"},{"location":"architectures/","title":"Model Architectures","text":"<p>This page provides an overview of all model architectures available in <code>timm</code>.</p>"},{"location":"architectures/#vision-transformers-vit","title":"Vision Transformers (ViT)","text":"<p>Transformer-based architectures for computer vision.</p>"},{"location":"architectures/#standard-vit","title":"Standard ViT","text":"<ul> <li>Vision Transformer - https://arxiv.org/abs/2010.11929</li> <li>Pure transformer architecture with patch embeddings</li> <li>Available in Tiny, Small, Base, Large, Huge sizes</li> </ul>"},{"location":"architectures/#vit-variants","title":"ViT Variants","text":"<ul> <li>DeiT - Data-efficient Image Transformers - https://arxiv.org/abs/2012.12877</li> <li>DeiT-III - https://arxiv.org/pdf/2204.07118.pdf</li> <li>BEiT - BERT Pre-Training for Image Transformers - https://arxiv.org/abs/2106.08254</li> <li>BEiT-V2 - https://arxiv.org/abs/2208.06366</li> <li>BEiT3 - https://arxiv.org/abs/2208.10442</li> <li>CaiT - Class-Attention in Image Transformers - https://arxiv.org/abs/2103.17239</li> <li>FlexiViT - https://arxiv.org/abs/2212.08013</li> <li>ROPE-ViT - https://arxiv.org/abs/2403.13298</li> <li>NaFlexViT - Native aspect ratio, variable resolution support</li> <li>PE (Perception Encoder) - https://arxiv.org/abs/2504.13181</li> </ul>"},{"location":"architectures/#clip-vision-encoders","title":"CLIP Vision Encoders","text":"<ul> <li>SigLIP (image encoder) - https://arxiv.org/abs/2303.15343</li> <li>SigLIP 2 (image encoder) - https://arxiv.org/abs/2502.14786</li> <li>MobileCLIP - https://arxiv.org/abs/2311.17049</li> <li>ViTamin - https://arxiv.org/abs/2404.02132</li> <li>TinyCLIP - Compact CLIP vision towers</li> </ul>"},{"location":"architectures/#hierarchical-transformers","title":"Hierarchical Transformers","text":"<p>Transformer architectures with hierarchical feature maps.</p>"},{"location":"architectures/#swin-transformer-family","title":"Swin Transformer Family","text":"<ul> <li>Swin Transformer - https://arxiv.org/abs/2103.14030</li> <li>Swin Transformer V2 - https://arxiv.org/abs/2111.09883</li> <li>Swin S3 (AutoFormerV2) - https://arxiv.org/abs/2111.14725</li> </ul>"},{"location":"architectures/#other-hierarchical","title":"Other Hierarchical","text":"<ul> <li>Twins - Spatial Attention in Vision Transformers - https://arxiv.org/pdf/2104.13840.pdf</li> <li>Focal Net - Focal Modulation Networks - https://arxiv.org/abs/2203.11926</li> <li>GCViT - Global Context Vision Transformer - https://arxiv.org/abs/2206.09959</li> <li>MaxViT - Multi-Axis Vision Transformer - https://arxiv.org/abs/2204.01697</li> <li>CoaT - Co-Scale Conv-Attentional Image Transformers - https://arxiv.org/abs/2104.06399</li> </ul>"},{"location":"architectures/#hybrid-conv-transformer-models","title":"Hybrid Conv-Transformer Models","text":"<p>Combining convolutional and transformer components.</p>"},{"location":"architectures/#major-hybrids","title":"Major Hybrids","text":"<ul> <li>CoAtNet - Convolution and Attention - https://arxiv.org/abs/2106.04803</li> <li>ConViT - Soft Convolutional Inductive Biases - https://arxiv.org/abs/2103.10697</li> <li>LeViT - Vision Transformer in ConvNet's Clothing - https://arxiv.org/abs/2104.01136</li> <li>MobileViT - https://arxiv.org/abs/2110.02178</li> <li>MobileViT-V2 - https://arxiv.org/abs/2206.02680</li> <li>EfficientViT (MIT) - https://arxiv.org/abs/2205.14756</li> <li>EfficientViT (MSRA) - https://arxiv.org/abs/2305.07027</li> <li>FastViT - https://arxiv.org/abs/2303.14189</li> <li>Next-ViT - https://arxiv.org/abs/2207.05501</li> </ul>"},{"location":"architectures/#modern-convnets","title":"Modern ConvNets","text":"<p>Pure convolutional architectures with modern design principles.</p>"},{"location":"architectures/#convnext-family","title":"ConvNeXt Family","text":"<ul> <li>ConvNeXt - https://arxiv.org/abs/2201.03545</li> <li>ConvNeXt-V2 - http://arxiv.org/abs/2301.00808</li> <li>Modern ConvNet design inspired by transformers</li> </ul>"},{"location":"architectures/#recent-convnets","title":"Recent ConvNets","text":"<ul> <li>MambaOut - https://arxiv.org/abs/2405.07992</li> <li>FasterNet - https://arxiv.org/abs/2303.03667</li> <li>SwiftFormer - https://arxiv.org/pdf/2303.15446</li> <li>StarNet - https://arxiv.org/abs/2403.19967</li> <li>RDNet - DenseNets Reloaded - https://arxiv.org/abs/2403.19588</li> <li>EdgeNeXt - https://arxiv.org/abs/2206.10589</li> <li>InceptionNeXt - https://arxiv.org/abs/2303.16900</li> </ul>"},{"location":"architectures/#mobile-efficient-models","title":"Mobile &amp; Efficient Models","text":"<p>Architectures optimized for mobile and edge deployment.</p>"},{"location":"architectures/#mobilenet-family","title":"MobileNet Family","text":"<ul> <li>MobileNet-V1 - Depthwise separable convolutions</li> <li>MobileNet-V2 - https://arxiv.org/abs/1801.04381</li> <li>MobileNet-V3 - https://arxiv.org/abs/1905.02244</li> <li>MobileNet-V4 - https://arxiv.org/abs/2404.10518</li> <li>MobileNet-V5 - Backbone for Gemma 3n</li> </ul>"},{"location":"architectures/#efficientnet-family","title":"EfficientNet Family","text":"<ul> <li>EfficientNet (B0-B7) - https://arxiv.org/abs/1905.11946</li> <li>EfficientNet-EdgeTPU - https://ai.googleblog.com/2019/08/efficientnet-edgetpu-creating.html</li> <li>EfficientNet V2 - https://arxiv.org/abs/2104.00298</li> <li>EfficientNet NoisyStudent - https://arxiv.org/abs/1911.04252</li> <li>EfficientNet AdvProp - https://arxiv.org/abs/1911.09665</li> </ul>"},{"location":"architectures/#other-mobile-models","title":"Other Mobile Models","text":"<ul> <li>GhostNet - https://arxiv.org/abs/1911.11907</li> <li>GhostNet-V2 - https://arxiv.org/abs/2211.12905</li> <li>GhostNet-V3 - https://arxiv.org/abs/2404.11202</li> <li>RepGhostNet - https://arxiv.org/abs/2211.06088</li> <li>MobileOne - https://arxiv.org/abs/2206.04040</li> <li>RepViT - https://arxiv.org/abs/2307.09283</li> <li>TinyViT - https://arxiv.org/abs/2207.10666</li> <li>SHViT - https://arxiv.org/abs/2401.16456</li> <li>MNASNet - https://arxiv.org/abs/1807.11626</li> <li>FBNet-C - https://arxiv.org/abs/1812.03443</li> <li>MixNet - https://arxiv.org/abs/1907.09595</li> <li>TinyNet - https://arxiv.org/abs/2010.14819</li> <li>Single-Path NAS - https://arxiv.org/abs/1904.02877</li> <li>HardCoRe-NAS - https://arxiv.org/abs/2102.11646</li> <li>LCNet - https://arxiv.org/abs/2109.15099</li> </ul>"},{"location":"architectures/#resnet-variants","title":"ResNet &amp; Variants","text":"<p>Classic and improved residual network architectures.</p>"},{"location":"architectures/#standard-resnet","title":"Standard ResNet","text":"<ul> <li>ResNet (v1b/v1.5) - https://arxiv.org/abs/1512.03385</li> <li>ResNeXt - https://arxiv.org/abs/1611.05431</li> <li>Big Transfer ResNetV2 (BiT) - https://arxiv.org/abs/1912.11370</li> </ul>"},{"location":"architectures/#resnet-improvements","title":"ResNet Improvements","text":"<ul> <li>ResNet-D - Bag of Tricks - https://arxiv.org/abs/1812.01187</li> <li>ResNet-RS - https://arxiv.org/abs/2103.07579</li> <li>ECA-ResNet - https://arxiv.org/abs/1910.03151v4</li> <li>SE-ResNet - Squeeze-and-Excitation - https://arxiv.org/abs/1709.01507</li> <li>SK-ResNet - Selective Kernel - https://arxiv.org/abs/1903.06586</li> <li>Res2Net - https://arxiv.org/abs/1904.01169</li> <li>ResNeSt - https://arxiv.org/abs/2004.08955</li> </ul>"},{"location":"architectures/#pre-trained-resnet-variants","title":"Pre-trained ResNet Variants","text":"<ul> <li>WSL - Weakly-supervised Instagram pretrained - https://arxiv.org/abs/1805.00932</li> <li>SSL/SWSL - Semi-supervised/Semi-weakly supervised - https://arxiv.org/abs/1905.00546</li> </ul>"},{"location":"architectures/#attention-meta-former-models","title":"Attention &amp; Meta-Former Models","text":"<p>Models based on attention mechanisms or meta-architectures.</p>"},{"location":"architectures/#pure-attention","title":"Pure Attention","text":"<ul> <li>Bottleneck Transformers - https://arxiv.org/abs/2101.11605</li> <li>Lambda Networks - https://arxiv.org/abs/2102.08602</li> <li>Halo Nets - https://arxiv.org/abs/2103.12731</li> </ul>"},{"location":"architectures/#meta-formers","title":"Meta-Formers","text":"<ul> <li>MetaFormer - PoolFormer-v2, ConvFormer, CAFormer - https://arxiv.org/abs/2210.13452</li> <li>PoolFormer - https://arxiv.org/abs/2111.11418</li> </ul>"},{"location":"architectures/#mlp-based","title":"MLP-Based","text":"<ul> <li>MLP-Mixer - https://arxiv.org/abs/2105.01601</li> <li>ResMLP - https://arxiv.org/abs/2105.03404</li> <li>gMLP - https://arxiv.org/abs/2105.08050</li> <li>Sequencer2D - https://arxiv.org/abs/2205.01972</li> </ul>"},{"location":"architectures/#specialized-architectures","title":"Specialized Architectures","text":""},{"location":"architectures/#multi-scale-pyramids","title":"Multi-Scale &amp; Pyramids","text":"<ul> <li>PVT-V2 - Improved Pyramid Vision Transformer - https://arxiv.org/abs/2106.13797</li> <li>PiT - Pooling-based Vision Transformer - https://arxiv.org/abs/2103.16302</li> <li>MViT-V2 - Improved Multiscale Vision Transformer - https://arxiv.org/abs/2112.01526</li> <li>NesT - Aggregating Nested Transformers - https://arxiv.org/abs/2105.12723</li> <li>TNT - Transformer-iN-Transformer - https://arxiv.org/abs/2103.00112</li> </ul>"},{"location":"architectures/#eva-models","title":"EVA Models","text":"<ul> <li>EVA - https://arxiv.org/abs/2211.07636</li> <li>EVA-02 - https://arxiv.org/abs/2303.11331</li> </ul>"},{"location":"architectures/#davit-hiera","title":"DaViT &amp; Hiera","text":"<ul> <li>DaViT - Dual Attention Vision Transformers</li> <li>Hiera - Hierarchical vision transformer from Meta - https://github.com/facebookresearch/hiera</li> </ul>"},{"location":"architectures/#classic-legacy-architectures","title":"Classic &amp; Legacy Architectures","text":"<p>Time-tested architectures still widely used.</p>"},{"location":"architectures/#dense-efficient","title":"Dense &amp; Efficient","text":"<ul> <li>DenseNet - https://arxiv.org/abs/1608.06993</li> <li>DPN - Dual-Path Network - https://arxiv.org/abs/1707.01629</li> <li>GPU-Efficient Networks - https://arxiv.org/abs/2006.14090</li> </ul>"},{"location":"architectures/#inception-family","title":"Inception Family","text":"<ul> <li>Inception-V3 - https://arxiv.org/abs/1512.00567</li> <li>Inception-ResNet-V2 / Inception-V4 - https://arxiv.org/abs/1602.07261</li> <li>Xception - https://arxiv.org/abs/1610.02357</li> <li>Xception (Modified Aligned) - https://arxiv.org/abs/1802.02611</li> </ul>"},{"location":"architectures/#detection-backbones","title":"Detection Backbones","text":"<ul> <li>DLA - Deep Layer Aggregation - https://arxiv.org/abs/1707.06484</li> <li>HRNet - High-Resolution Net - https://arxiv.org/abs/1908.07919</li> <li>CSPNet - Cross-Stage Partial Networks - https://arxiv.org/abs/1911.11929</li> <li>TResNet - https://arxiv.org/abs/2003.13630</li> <li>VovNet V2 and V1 - https://arxiv.org/abs/1911.06667</li> </ul>"},{"location":"architectures/#specialized","title":"Specialized","text":"<ul> <li>SelecSLS - https://arxiv.org/abs/1907.00837</li> <li>ReXNet - https://arxiv.org/abs/2007.00992</li> <li>ResMLP - https://arxiv.org/abs/2105.03404</li> <li>RepVGG - https://arxiv.org/abs/2101.03697</li> <li>SqueezeNet - Lightweight architecture</li> <li>VGG - https://arxiv.org/abs/1409.1556</li> </ul>"},{"location":"architectures/#nas-models","title":"NAS Models","text":"<ul> <li>NASNet-A - https://arxiv.org/abs/1707.07012</li> <li>PNasNet - https://arxiv.org/abs/1712.00559</li> <li>EfficientNet (AutoML) - https://arxiv.org/abs/1905.11946</li> </ul>"},{"location":"architectures/#normalizer-free","title":"Normalizer-Free","text":"<ul> <li>NFNet-F - https://arxiv.org/abs/2102.06171</li> <li>NF-RegNet / NF-ResNet - https://arxiv.org/abs/2101.08692</li> </ul>"},{"location":"architectures/#other-notable","title":"Other Notable","text":"<ul> <li>RegNet - https://arxiv.org/abs/2003.13678</li> <li>RegNetZ - https://arxiv.org/abs/2103.06877</li> <li>VOLO - Vision Outlooker - https://arxiv.org/abs/2106.13112</li> <li>XCiT - Cross-Covariance Image Transformers - https://arxiv.org/abs/2106.09681</li> <li>Visformer - https://arxiv.org/abs/2104.12533</li> <li>HGNet / HGNet-V2 - From PaddlePaddle</li> </ul>"},{"location":"architectures/#architecture-categories","title":"Architecture Categories","text":""},{"location":"architectures/#by-paradigm","title":"By Paradigm","text":"<ol> <li>Pure Transformers: ViT, DeiT, BEiT</li> <li>Hierarchical Transformers: Swin, PVT, Twins</li> <li>Hybrid: CoAtNet, LeViT, MobileViT</li> <li>Pure ConvNets: ResNet, EfficientNet, ConvNeXt</li> <li>MLP-Based: MLP-Mixer, ResMLP, gMLP</li> <li>Meta-Architectures: MetaFormer, PoolFormer</li> </ol>"},{"location":"architectures/#by-use-case","title":"By Use Case","text":"<ul> <li>Mobile/Edge: MobileNet series, GhostNet, RepViT</li> <li>Server/Cloud: Large ViT, EVA, Swin-Large</li> <li>Balanced: EfficientNet, ResNet-50, ViT-Base</li> <li>Research: Experimental architectures, NAS models</li> </ul>"},{"location":"architectures/#by-training-type","title":"By Training Type","text":"<ul> <li>Supervised: Standard ImageNet training</li> <li>Self-Supervised: BEiT, DINO variants</li> <li>Weakly-Supervised: WSL ResNets</li> <li>Distillation: DeiT models</li> <li>CLIP-style: SigLIP, MobileCLIP</li> </ul>"},{"location":"architectures/#architecture-selection-guide","title":"Architecture Selection Guide","text":"<p>Choose based on your constraints:</p> <ul> <li>Accuracy Priority: Large ViT, EVA, Swin-Large</li> <li>Speed Priority: MobileNet-V4, EfficientNet-B0, FastViT</li> <li>Memory Priority: Tiny models, MobileNet variants</li> <li>Balanced: ResNet-50D, EfficientNet-B1-B3, ViT-Base</li> <li>Novel Features: NaFlexViT (variable aspect), ROPE-ViT (rotary embeddings)</li> </ul>"},{"location":"architectures/#links","title":"Links","text":"<ul> <li>Model Metrics</li> <li>Getting Started</li> <li>GitHub Repository</li> </ul>"},{"location":"features/","title":"Features","text":"<p><code>timm</code> provides a comprehensive set of features for computer vision research and production.</p>"},{"location":"features/#core-features","title":"Core Features","text":""},{"location":"features/#1000-pre-trained-models","title":"1000+ Pre-trained Models","text":"<p>Access to a vast collection of state-of-the-art models:</p> <pre><code>import timm\n\n# List all available models\nall_models = timm.list_models()\nprint(f\"Total models: {len(all_models)}\")\n\n# Search for specific models\nresnet_models = timm.list_models('resnet*')\nvit_models = timm.list_models('vit_*')\n</code></pre>"},{"location":"features/#consistent-model-api","title":"Consistent Model API","text":"<p>All models share a common interface:</p> <pre><code># Create any model with pretrained weights\nmodel = timm.create_model('resnet50', pretrained=True)\n\n# Access/change classifier\nnum_classes = model.get_classifier()\nmodel.reset_classifier(num_classes=10)\n\n# Feature extraction\nfeatures = model.forward_features(x)\n</code></pre>"},{"location":"features/#multi-scale-feature-extraction","title":"Multi-Scale Feature Extraction","text":"<p>Easy feature pyramid extraction from any model:</p> <pre><code># Create model for feature extraction\nmodel = timm.create_model(\n    'resnet50',\n    features_only=True,\n    out_indices=(1, 2, 3, 4)\n)\n\n# Get multi-scale features\nfeatures = model(x)\nfor feat in features:\n    print(feat.shape)  # Different spatial resolutions\n</code></pre>"},{"location":"features/#flexible-image-preprocessing","title":"Flexible Image Preprocessing","text":"<p>Built-in data configuration for each model:</p> <pre><code># Get model-specific preprocessing config\ndata_config = timm.data.resolve_data_config({}, model=model)\ntransform = timm.data.create_transform(**data_config)\n\n# Apply preprocessing\npreprocessed = transform(image)\n</code></pre>"},{"location":"features/#model-features","title":"Model Features","text":""},{"location":"features/#adaptive-weight-loading","title":"Adaptive Weight Loading","text":"<p>Automatically adapts weights to different configurations:</p> <pre><code># Load pretrained weights with different classifier\nmodel = timm.create_model(\n    'resnet50',\n    pretrained=True,\n    num_classes=100  # Adapts final layer\n)\n\n# Load with different input channels\nmodel = timm.create_model(\n    'resnet50',\n    pretrained=True,\n    in_chans=1  # Converts RGB weights to grayscale\n)\n</code></pre>"},{"location":"features/#dynamic-resolution","title":"Dynamic Resolution","text":"<p>Many models support different input sizes:</p> <pre><code># Standard resolution\nmodel = timm.create_model('vit_base_patch16_224', img_size=224)\n\n# Higher resolution\nmodel = timm.create_model('vit_base_patch16_224', img_size=384)\n</code></pre>"},{"location":"features/#test-time-augmentation","title":"Test Time Augmentation","text":"<p>Wrapper for improved inference with larger images:</p> <pre><code>from timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\n# Create model with test-time pooling\nmodel = timm.create_model('resnet50', pretrained=True)\nconfig = resolve_data_config({}, model=model)\n\n# Use larger input size at test time\nconfig['input_size'] = (3, 288, 288)\ntransform = create_transform(**config)\n</code></pre>"},{"location":"features/#optimizers","title":"Optimizers","text":"<p>Extensive selection of modern optimizers via <code>timm.optim.create_optimizer_v2</code>:</p> <pre><code>import timm.optim\n\n# List all available optimizers\noptimizers = timm.optim.list_optimizers()\n\n# Create optimizer\noptimizer = timm.optim.create_optimizer_v2(\n    model,\n    opt='adamw',\n    lr=1e-3,\n    weight_decay=0.05\n)\n</code></pre>"},{"location":"features/#available-optimizers","title":"Available Optimizers","text":"<ul> <li>AdamW: Adam with decoupled weight decay</li> <li>LAMB: Layer-wise Adaptive Moments optimizer</li> <li>LARS: Layer-wise Adaptive Rate Scaling</li> <li>Lion: Evolved optimizer from Google</li> <li>Adafactor: Memory-efficient adaptive optimizer</li> <li>AdaBelief: Adapting stepsizes by belief in gradients</li> <li>Adan: Adaptive Nesterov momentum</li> <li>ADOPT: Adaptive gradient methods</li> <li>MARS: Modern adaptive optimizer</li> <li>LaProp: Layer-wise adaptive learning rates</li> <li>Muon: Orthogonalization-based optimizer</li> <li>Kron: Kronecker-factored preconditioner</li> </ul>"},{"location":"features/#augmentations","title":"Augmentations","text":"<p>State-of-the-art data augmentation techniques:</p>"},{"location":"features/#random-erasing","title":"Random Erasing","text":"<pre><code>from timm.data import RandomErasing\n\ntransform = RandomErasing(\n    probability=0.25,\n    mode='pixel',\n    max_count=1\n)\n</code></pre>"},{"location":"features/#mixup-cutmix","title":"Mixup &amp; CutMix","text":"<pre><code>from timm.data import Mixup\n\nmixup_fn = Mixup(\n    mixup_alpha=0.8,\n    cutmix_alpha=1.0,\n    prob=1.0,\n    label_smoothing=0.1,\n    num_classes=1000\n)\n</code></pre>"},{"location":"features/#autoaugment-randaugment","title":"AutoAugment &amp; RandAugment","text":"<pre><code>from timm.data.auto_augment import rand_augment_transform\n\ntransform = rand_augment_transform(\n    config_str='rand-m9-mstd0.5',\n    hparams={'translate_const': 100}\n)\n</code></pre>"},{"location":"features/#regularization","title":"Regularization","text":""},{"location":"features/#droppath-stochastic-depth","title":"DropPath (Stochastic Depth)","text":"<pre><code>from timm.models.layers import DropPath\n\ndrop_path = DropPath(drop_prob=0.1)\n</code></pre>"},{"location":"features/#dropblock","title":"DropBlock","text":"<pre><code>from timm.models.layers import DropBlock2d\n\ndrop_block = DropBlock2d(\n    drop_prob=0.1,\n    block_size=7\n)\n</code></pre>"},{"location":"features/#blur-pooling","title":"Blur Pooling","text":"<p>Anti-aliased downsampling for better shift-invariance:</p> <pre><code># Many models support anti-aliased pooling\nmodel = timm.create_model(\n    'resnet50',\n    aa_layer=True  # Enable blur pooling\n)\n</code></pre>"},{"location":"features/#learning-rate-schedulers","title":"Learning Rate Schedulers","text":"<p>Flexible scheduling with warmup and restarts:</p> <pre><code>from timm.scheduler import CosineLRScheduler\n\nscheduler = CosineLRScheduler(\n    optimizer,\n    t_initial=300,  # epochs\n    lr_min=1e-6,\n    warmup_t=5,\n    warmup_lr_init=1e-6,\n    cycle_limit=1\n)\n</code></pre>"},{"location":"features/#available-schedulers","title":"Available Schedulers","text":"<ul> <li>Cosine: Cosine annealing with warm restarts</li> <li>Step: Step decay at milestones</li> <li>Plateau: Reduce on plateau</li> <li>Tanh: Tanh decay with restarts</li> <li>Polynomial: Polynomial decay</li> </ul>"},{"location":"features/#training-utilities","title":"Training Utilities","text":""},{"location":"features/#model-ema","title":"Model EMA","text":"<p>Exponential moving average of model weights:</p> <pre><code>from timm.utils import ModelEmaV2\n\nmodel_ema = ModelEmaV2(\n    model,\n    decay=0.9999,\n    device=device\n)\n\n# Update during training\nmodel_ema.update(model)\n\n# Use for validation\nwith torch.no_grad():\n    output = model_ema.module(input)\n</code></pre>"},{"location":"features/#gradient-clipping","title":"Gradient Clipping","text":"<p>Adaptive gradient clipping from NFNets:</p> <pre><code>from timm.utils import dispatch_clip_grad\n\ndispatch_clip_grad(\n    model.parameters(),\n    value=1.0,\n    mode='agc'  # Adaptive Gradient Clipping\n)\n</code></pre>"},{"location":"features/#mixed-precision-training","title":"Mixed Precision Training","text":"<p>Native PyTorch AMP support:</p> <pre><code>from torch.cuda.amp import autocast, GradScaler\n\nscaler = GradScaler()\n\nfor inputs, targets in loader:\n    optimizer.zero_grad()\n\n    with autocast():\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n</code></pre>"},{"location":"features/#attention-modules","title":"Attention Modules","text":"<p>Extensive selection of attention mechanisms:</p> <ul> <li>Squeeze-and-Excitation (SE): Channel attention</li> <li>Effective SE (ESE): Efficient variant</li> <li>CBAM: Convolutional Block Attention Module</li> <li>ECA: Efficient Channel Attention</li> <li>Global Context (GC): Global context modeling</li> <li>Gather-Excite (GE): Spatial-channel attention</li> <li>Selective Kernel (SK): Dynamic convolution selection</li> <li>SPLAT: Split attention</li> <li>Coordinate Attention: Position-aware attention</li> </ul>"},{"location":"features/#layer-types","title":"Layer Types","text":"<p>Rich collection of layer implementations:</p>"},{"location":"features/#convolution-variants","title":"Convolution Variants","text":"<ul> <li>Standard Conv2d with same padding</li> <li>Depthwise separable convolutions</li> <li>Mixed depthwise convolutions</li> <li>Blur pooling</li> <li>Space-to-depth transforms</li> </ul>"},{"location":"features/#normalization","title":"Normalization","text":"<ul> <li>BatchNorm with sync support</li> <li>GroupNorm</li> <li>LayerNorm (2D)</li> <li>RMSNorm</li> <li>EvoNorm</li> </ul>"},{"location":"features/#activation-functions","title":"Activation Functions","text":"<ul> <li>All PyTorch activations</li> <li>Swish / SiLU</li> <li>Mish</li> <li>Hard Swish</li> <li>Hard Sigmoid</li> <li>GELU variants</li> </ul>"},{"location":"features/#utilities","title":"Utilities","text":""},{"location":"features/#model-surgery","title":"Model Surgery","text":"<pre><code># Freeze layers\nfor name, param in model.named_parameters():\n    if 'layer4' not in name:\n        param.requires_grad = False\n\n# Replace layers\nmodel.fc = nn.Linear(2048, num_classes)\n</code></pre>"},{"location":"features/#feature-extraction","title":"Feature Extraction","text":"<pre><code># Extract intermediate features\nmodel = timm.create_model('resnet50', features_only=True)\nfeatures = model(x)\n\n# Or use forward_intermediates\nmodel = timm.create_model('vit_base_patch16_224')\nfinal_feat, intermediates = model.forward_intermediates(x)\n</code></pre>"},{"location":"features/#model-information","title":"Model Information","text":"<pre><code># Get model metadata\nmodel_info = timm.models.get_pretrained_cfg('resnet50')\n\n# Count parameters\nnum_params = sum(p.numel() for p in model.parameters())\n\n# Get feature info\nif hasattr(model, 'feature_info'):\n    for info in model.feature_info:\n        print(f\"Stage: {info['module']}, Channels: {info['num_chs']}\")\n</code></pre>"},{"location":"features/#production-features","title":"Production Features","text":""},{"location":"features/#onnx-export","title":"ONNX Export","text":"<pre><code># Export to ONNX\ndummy_input = torch.randn(1, 3, 224, 224)\ntorch.onnx.export(\n    model,\n    dummy_input,\n    \"model.onnx\",\n    input_names=['input'],\n    output_names=['output']\n)\n</code></pre>"},{"location":"features/#torchscript","title":"TorchScript","text":"<pre><code># Script model\nscripted = torch.jit.script(model)\n\n# Trace model\ntraced = torch.jit.trace(model, example_input)\n</code></pre>"},{"location":"features/#quantization-support","title":"Quantization Support","text":"<p>Many models support PyTorch quantization for inference speedup.</p>"},{"location":"features/#documentation-resources","title":"Documentation &amp; Resources","text":""},{"location":"features/#model-hub","title":"Model Hub","text":"<p>All pretrained weights are available on Hugging Face Hub:</p> <pre><code># Models are automatically downloaded from HF Hub\nmodel = timm.create_model('resnet50.a1_in1k', pretrained=True)\n\n# Or explicitly use hub:\nmodel = timm.create_model('hf-hub:timm/resnet50.a1_in1k', pretrained=True)\n</code></pre>"},{"location":"features/#papers-with-code","title":"Papers with Code","text":"<p>Browse models by task and performance: https://paperswithcode.com/lib/timm</p>"},{"location":"features/#official-documentation","title":"Official Documentation","text":"<p>Comprehensive guides at: https://huggingface.co/docs/timm</p>"},{"location":"features/#links","title":"Links","text":"<ul> <li>Model Architectures</li> <li>Model Metrics</li> <li>Getting Started</li> <li>Training Scripts</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide will help you get started with PyTorch Image Models (timm).</p>"},{"location":"getting-started/#installation","title":"Installation","text":""},{"location":"getting-started/#basic-installation","title":"Basic Installation","text":"<p>Install from PyPI:</p> <pre><code>pip install timm\n</code></pre>"},{"location":"getting-started/#development-installation","title":"Development Installation","text":"<p>Clone and install from source:</p> <pre><code>git clone https://github.com/GeorgePearse/pytorch-image-models\ncd pytorch-image-models\npip install -e .\n</code></pre>"},{"location":"getting-started/#requirements","title":"Requirements","text":"<ul> <li>Python &gt;= 3.8</li> <li>PyTorch &gt;= 2.0</li> <li>torchvision</li> </ul>"},{"location":"getting-started/#quick-start","title":"Quick Start","text":""},{"location":"getting-started/#loading-a-pretrained-model","title":"Loading a Pretrained Model","text":"<pre><code>import timm\nimport torch\n\n# Create a model with pretrained weights\nmodel = timm.create_model('resnet50', pretrained=True)\nmodel.eval()\n\n# Prepare input (batch_size=1, channels=3, height=224, width=224)\nx = torch.randn(1, 3, 224, 224)\n\n# Get predictions\nwith torch.no_grad():\n    output = model(x)\n\nprint(output.shape)  # torch.Size([1, 1000])\n</code></pre>"},{"location":"getting-started/#listing-available-models","title":"Listing Available Models","text":"<pre><code>import timm\n\n# List all models\nall_models = timm.list_models()\nprint(f\"Total models: {len(all_models)}\")\n\n# Search for specific models\nresnet_models = timm.list_models('resnet*')\nprint(f\"ResNet models: {len(resnet_models)}\")\n\n# List models with pretrained weights\npretrained_models = timm.list_models(pretrained=True)\n\n# Filter by specific characteristics\nvit_models = timm.list_models('vit_*', pretrained=True)\n</code></pre>"},{"location":"getting-started/#loading-with-custom-number-of-classes","title":"Loading with Custom Number of Classes","text":"<pre><code># For fine-tuning on custom dataset\nmodel = timm.create_model(\n    'resnet50',\n    pretrained=True,\n    num_classes=10  # Your number of classes\n)\n</code></pre>"},{"location":"getting-started/#image-preprocessing","title":"Image Preprocessing","text":""},{"location":"getting-started/#using-model-specific-transforms","title":"Using Model-Specific Transforms","text":"<pre><code>import timm\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\nfrom PIL import Image\n\n# Create model\nmodel = timm.create_model('resnet50', pretrained=True)\nmodel.eval()\n\n# Get model-specific preprocessing config\nconfig = resolve_data_config({}, model=model)\nprint(config)\n# {'input_size': (3, 224, 224), 'interpolation': 'bicubic',\n#  'mean': (0.485, 0.456, 0.406), 'std': (0.229, 0.224, 0.225), ...}\n\n# Create transform\ntransform = create_transform(**config)\n\n# Load and preprocess image\nimg = Image.open('path/to/image.jpg')\ntensor = transform(img).unsqueeze(0)  # Add batch dimension\n\n# Inference\nwith torch.no_grad():\n    output = model(tensor)\n</code></pre>"},{"location":"getting-started/#manual-preprocessing","title":"Manual Preprocessing","text":"<pre><code>from torchvision import transforms\n\n# Standard ImageNet preprocessing\ntransform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n])\n</code></pre>"},{"location":"getting-started/#feature-extraction","title":"Feature Extraction","text":""},{"location":"getting-started/#extract-features-only","title":"Extract Features Only","text":"<pre><code># Create model for feature extraction\nmodel = timm.create_model(\n    'resnet50',\n    pretrained=True,\n    num_classes=0,  # Remove classifier\n    global_pool=''   # Remove global pooling\n)\n\n# Get features\nfeatures = model.forward_features(x)\nprint(features.shape)  # torch.Size([1, 2048, 7, 7])\n</code></pre>"},{"location":"getting-started/#multi-scale-feature-extraction","title":"Multi-Scale Feature Extraction","text":"<pre><code># Create feature pyramid model\nmodel = timm.create_model(\n    'resnet50',\n    features_only=True,\n    out_indices=(1, 2, 3, 4)  # Get features from these stages\n)\n\n# Get multi-scale features\nfeatures = model(x)\n\nfor i, feat in enumerate(features):\n    print(f\"Feature {i}: {feat.shape}\")\n# Feature 0: torch.Size([1, 256, 56, 56])\n# Feature 1: torch.Size([1, 512, 28, 28])\n# Feature 2: torch.Size([1, 1024, 14, 14])\n# Feature 3: torch.Size([1, 2048, 7, 7])\n</code></pre>"},{"location":"getting-started/#using-forward_intermediates-vit-models","title":"Using forward_intermediates (ViT models)","text":"<pre><code>model = timm.create_model('vit_base_patch16_224', pretrained=True)\n\n# Get final features and all intermediate layer outputs\nfinal_feat, intermediates = model.forward_intermediates(x)\n\nprint(final_feat.shape)  # torch.Size([1, 197, 768])\n\nfor i, feat in enumerate(intermediates):\n    print(f\"Layer {i}: {feat.shape}\")\n</code></pre>"},{"location":"getting-started/#fine-tuning","title":"Fine-Tuning","text":""},{"location":"getting-started/#basic-fine-tuning-setup","title":"Basic Fine-Tuning Setup","text":"<pre><code>import torch\nimport torch.nn as nn\nimport timm\n\n# Load pretrained model\nmodel = timm.create_model(\n    'resnet50',\n    pretrained=True,\n    num_classes=10  # Your dataset classes\n)\n\n# Setup for fine-tuning\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\n# Training loop\nmodel.train()\nfor images, labels in train_loader:\n    optimizer.zero_grad()\n    outputs = model(images)\n    loss = criterion(outputs, labels)\n    loss.backward()\n    optimizer.step()\n</code></pre>"},{"location":"getting-started/#freeze-early-layers","title":"Freeze Early Layers","text":"<pre><code># Freeze all layers except final classifier\nfor name, param in model.named_parameters():\n    if 'fc' not in name:  # 'fc' is the classifier layer name\n        param.requires_grad = False\n\n# Only optimize classifier\noptimizer = torch.optim.Adam(\n    filter(lambda p: p.requires_grad, model.parameters()),\n    lr=1e-3\n)\n</code></pre>"},{"location":"getting-started/#progressive-unfreezing","title":"Progressive Unfreezing","text":"<pre><code># Freeze all layers initially\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Unfreeze classifier\nfor param in model.get_classifier().parameters():\n    param.requires_grad = True\n\n# Train classifier first...\n\n# Later, unfreeze more layers\nfor name, param in model.named_parameters():\n    if 'layer4' in name or 'fc' in name:\n        param.requires_grad = True\n</code></pre>"},{"location":"getting-started/#using-different-input-sizes","title":"Using Different Input Sizes","text":""},{"location":"getting-started/#runtime-resolution-change","title":"Runtime Resolution Change","text":"<pre><code># Create model with default resolution\nmodel = timm.create_model('resnet50', pretrained=True)\n\n# Use with different input size (must be divisible by 32 for ResNet)\nx_small = torch.randn(1, 3, 224, 224)\nx_large = torch.randn(1, 3, 448, 448)\n\noutput_small = model(x_small)\noutput_large = model(x_large)\n</code></pre>"},{"location":"getting-started/#set-input-size-at-creation","title":"Set Input Size at Creation","text":"<pre><code># For models that need specific input size (like ViT)\nmodel = timm.create_model(\n    'vit_base_patch16_224',\n    pretrained=True,\n    img_size=384  # Use 384x384 instead of 224x224\n)\n</code></pre>"},{"location":"getting-started/#working-with-different-input-channels","title":"Working with Different Input Channels","text":""},{"location":"getting-started/#single-channel-input","title":"Single Channel Input","text":"<pre><code># For grayscale images\nmodel = timm.create_model(\n    'resnet50',\n    pretrained=True,\n    in_chans=1  # Single channel\n)\n</code></pre>"},{"location":"getting-started/#multi-channel-input","title":"Multi-Channel Input","text":"<pre><code># For multi-spectral images\nmodel = timm.create_model(\n    'resnet50',\n    pretrained=True,\n    in_chans=4  # 4 channels\n)\n</code></pre>"},{"location":"getting-started/#gpu-usage","title":"GPU Usage","text":""},{"location":"getting-started/#single-gpu","title":"Single GPU","text":"<pre><code>device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\n\n# Inference\nx = x.to(device)\noutput = model(x)\n</code></pre>"},{"location":"getting-started/#multi-gpu-dataparallel","title":"Multi-GPU (DataParallel)","text":"<pre><code>if torch.cuda.device_count() &gt; 1:\n    model = nn.DataParallel(model)\nmodel = model.to('cuda')\n</code></pre>"},{"location":"getting-started/#distributed-training-ddp","title":"Distributed Training (DDP)","text":"<pre><code>import torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\n# Initialize process group\ndist.init_process_group(backend='nccl')\n\n# Create model and move to GPU\nmodel = model.to(local_rank)\nmodel = DDP(model, device_ids=[local_rank])\n</code></pre>"},{"location":"getting-started/#model-surgery","title":"Model Surgery","text":""},{"location":"getting-started/#replace-classifier","title":"Replace Classifier","text":"<pre><code>import torch.nn as nn\n\n# Get classifier\nclassifier = model.get_classifier()\nprint(classifier)\n\n# Replace with custom head\nmodel.reset_classifier(num_classes=100)\n\n# Or manually replace\nmodel.fc = nn.Sequential(\n    nn.Dropout(0.5),\n    nn.Linear(2048, 100)\n)\n</code></pre>"},{"location":"getting-started/#add-auxiliary-heads","title":"Add Auxiliary Heads","text":"<pre><code>class ModelWithAuxHead(nn.Module):\n    def __init__(self, base_model, num_classes=10):\n        super().__init__()\n        self.base = base_model\n        self.aux_head = nn.Linear(1024, num_classes)\n\n    def forward(self, x):\n        features = self.base.forward_features(x)\n        # Main output\n        main_out = self.base.forward_head(features)\n        # Auxiliary output from intermediate features\n        aux_out = self.aux_head(features.mean(dim=(2, 3)))\n        return main_out, aux_out\n</code></pre>"},{"location":"getting-started/#inference-tips","title":"Inference Tips","text":""},{"location":"getting-started/#batch-inference","title":"Batch Inference","text":"<pre><code>import torch.nn.functional as F\n\nmodel.eval()\nall_predictions = []\n\nwith torch.no_grad():\n    for batch in data_loader:\n        images = batch.to(device)\n        logits = model(images)\n        probs = F.softmax(logits, dim=1)\n        predictions = torch.argmax(probs, dim=1)\n        all_predictions.extend(predictions.cpu().numpy())\n</code></pre>"},{"location":"getting-started/#mixed-precision-inference","title":"Mixed Precision Inference","text":"<pre><code>from torch.cuda.amp import autocast\n\nmodel.eval()\nwith torch.no_grad(), autocast():\n    output = model(x.cuda())\n</code></pre>"},{"location":"getting-started/#test-time-augmentation","title":"Test Time Augmentation","text":"<pre><code>import torch.nn.functional as F\n\ndef tta_inference(model, image, num_augments=5):\n    \"\"\"Test time augmentation with random crops and flips\"\"\"\n    model.eval()\n    predictions = []\n\n    with torch.no_grad():\n        # Original\n        pred = model(image)\n        predictions.append(F.softmax(pred, dim=1))\n\n        # Horizontal flip\n        pred = model(torch.flip(image, dims=[3]))\n        predictions.append(F.softmax(pred, dim=1))\n\n        # Random crops (example)\n        for _ in range(num_augments - 2):\n            # Apply random transformations\n            pred = model(image)\n            predictions.append(F.softmax(pred, dim=1))\n\n    # Average predictions\n    avg_pred = torch.stack(predictions).mean(dim=0)\n    return avg_pred\n</code></pre>"},{"location":"getting-started/#common-issues","title":"Common Issues","text":""},{"location":"getting-started/#out-of-memory","title":"Out of Memory","text":"<pre><code># Reduce batch size\nbatch_size = 16  # Try smaller values\n\n# Use gradient checkpointing (for supported models)\nmodel.set_grad_checkpointing(enable=True)\n\n# Use mixed precision\nfrom torch.cuda.amp import autocast\nwith autocast():\n    output = model(x)\n</code></pre>"},{"location":"getting-started/#slow-training","title":"Slow Training","text":"<pre><code># Use DataLoader with multiple workers\ntrain_loader = torch.utils.data.DataLoader(\n    dataset,\n    batch_size=32,\n    num_workers=4,  # Adjust based on CPU cores\n    pin_memory=True\n)\n\n# Enable cuDNN benchmarking\ntorch.backends.cudnn.benchmark = True\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Explore Model Architectures</li> <li>Check Model Performance</li> <li>Learn About Features</li> <li>Read Training Scripts Guide</li> </ul>"},{"location":"getting-started/#additional-resources","title":"Additional Resources","text":"<ul> <li>Official timm Documentation</li> <li>Papers with Code</li> <li>Hugging Face Model Hub</li> </ul>"},{"location":"training/","title":"Training Scripts","text":"<p>The <code>timm</code> repository includes reference training, validation, and inference scripts for ImageNet and other datasets.</p>"},{"location":"training/#overview","title":"Overview","text":"<p>The training scripts are located in the root of the repository and support:</p> <ul> <li>Multi-GPU training with DistributedDataParallel</li> <li>Mixed precision training with native PyTorch AMP</li> <li>Modern augmentations (RandAugment, AutoAugment, CutMix, Mixup)</li> <li>Advanced regularization (DropPath, DropBlock, Label Smoothing)</li> <li>Flexible learning rate scheduling</li> <li>Model EMA (Exponential Moving Average)</li> <li>Gradient accumulation</li> <li>Checkpointing and resuming</li> </ul>"},{"location":"training/#training-script","title":"Training Script","text":""},{"location":"training/#basic-usage","title":"Basic Usage","text":"<pre><code>python train.py /path/to/imagenet \\\n  --model resnet50 \\\n  --batch-size 128 \\\n  --lr 0.1 \\\n  --epochs 100 \\\n  --amp\n</code></pre>"},{"location":"training/#common-arguments","title":"Common Arguments","text":""},{"location":"training/#model-configuration","title":"Model Configuration","text":"<pre><code>--model resnet50              # Model architecture\n--pretrained                  # Start from pretrained weights\n--num-classes 1000           # Number of output classes\n--img-size 224               # Input image size\n--in-chans 3                 # Number of input channels\n</code></pre>"},{"location":"training/#data-configuration","title":"Data Configuration","text":"<pre><code>--data-dir /path/to/data     # Dataset directory\n--dataset ImageFolder        # Dataset type\n--train-split train          # Training split name\n--val-split val              # Validation split name\n--batch-size 128             # Batch size per GPU\n--workers 4                  # Data loading workers\n</code></pre>"},{"location":"training/#optimizer-settings","title":"Optimizer Settings","text":"<pre><code>--opt adamw                  # Optimizer (adamw, sgd, lamb, etc.)\n--lr 0.001                   # Base learning rate\n--weight-decay 0.05          # Weight decay\n--momentum 0.9               # Momentum (for SGD)\n--clip-grad 1.0              # Gradient clipping threshold\n--clip-mode norm             # Clipping mode (norm, value, agc)\n</code></pre>"},{"location":"training/#learning-rate-schedule","title":"Learning Rate Schedule","text":"<pre><code>--sched cosine               # LR scheduler (cosine, step, plateau)\n--epochs 300                 # Number of training epochs\n--warmup-epochs 5            # Warmup epochs\n--warmup-lr 1e-6            # Warmup learning rate\n--min-lr 1e-6               # Minimum learning rate\n--decay-rate 0.1            # Decay rate for step scheduler\n</code></pre>"},{"location":"training/#augmentation","title":"Augmentation","text":"<pre><code>--aa rand-m9-mstd0.5        # AutoAugment policy\n--reprob 0.25               # Random erasing probability\n--remode pixel              # Random erasing mode\n--mixup 0.8                 # Mixup alpha\n--cutmix 1.0                # CutMix alpha\n--mixup-prob 1.0            # Probability of mixup/cutmix\n--smoothing 0.1             # Label smoothing\n</code></pre>"},{"location":"training/#regularization","title":"Regularization","text":"<pre><code>--drop 0.0                  # Dropout rate\n--drop-path 0.1             # DropPath rate\n--drop-block 0.0            # DropBlock rate\n</code></pre>"},{"location":"training/#training-features","title":"Training Features","text":"<pre><code>--amp                       # Use mixed precision training\n--channels-last             # Use channels last memory format\n--model-ema                 # Track EMA of model weights\n--model-ema-decay 0.9998   # EMA decay rate\n</code></pre>"},{"location":"training/#checkpointing","title":"Checkpointing","text":"<pre><code>--output /path/to/output    # Output directory\n--checkpoint-hist 10        # Keep last N checkpoints\n--resume /path/to/ckpt     # Resume from checkpoint\n</code></pre>"},{"location":"training/#advanced-examples","title":"Advanced Examples","text":""},{"location":"training/#training-resnet-50-with-modern-recipe","title":"Training ResNet-50 with Modern Recipe","text":"<pre><code>python train.py /imagenet \\\n  --model resnet50 \\\n  --batch-size 128 \\\n  --opt adamw \\\n  --lr 0.001 \\\n  --weight-decay 0.05 \\\n  --sched cosine \\\n  --epochs 300 \\\n  --warmup-epochs 5 \\\n  --aa rand-m9-mstd0.5 \\\n  --mixup 0.8 \\\n  --cutmix 1.0 \\\n  --smoothing 0.1 \\\n  --drop-path 0.1 \\\n  --amp \\\n  --model-ema \\\n  --workers 8\n</code></pre>"},{"location":"training/#training-vision-transformer","title":"Training Vision Transformer","text":"<pre><code>python train.py /imagenet \\\n  --model vit_base_patch16_224 \\\n  --batch-size 256 \\\n  --opt adamw \\\n  --lr 0.001 \\\n  --weight-decay 0.05 \\\n  --sched cosine \\\n  --epochs 300 \\\n  --warmup-epochs 5 \\\n  --warmup-lr 1e-6 \\\n  --min-lr 1e-5 \\\n  --aa rand-m9-mstd0.5-inc1 \\\n  --reprob 0.25 \\\n  --mixup 0.8 \\\n  --cutmix 1.0 \\\n  --smoothing 0.1 \\\n  --drop-path 0.1 \\\n  --amp \\\n  --model-ema \\\n  --model-ema-decay 0.99996\n</code></pre>"},{"location":"training/#fine-tuning-on-custom-dataset","title":"Fine-tuning on Custom Dataset","text":"<pre><code>python train.py /path/to/custom/dataset \\\n  --model resnet50 \\\n  --pretrained \\\n  --num-classes 10 \\\n  --batch-size 64 \\\n  --opt adamw \\\n  --lr 0.0001 \\\n  --weight-decay 0.01 \\\n  --epochs 50 \\\n  --warmup-epochs 3 \\\n  --aa rand-m7-mstd0.5 \\\n  --mixup 0.2 \\\n  --cutmix 0.0 \\\n  --smoothing 0.1 \\\n  --amp\n</code></pre>"},{"location":"training/#validation-script","title":"Validation Script","text":""},{"location":"training/#basic-usage_1","title":"Basic Usage","text":"<pre><code>python validate.py /path/to/imagenet \\\n  --model resnet50 \\\n  --checkpoint /path/to/checkpoint.pth \\\n  --batch-size 256 \\\n  --workers 4\n</code></pre>"},{"location":"training/#common-arguments_1","title":"Common Arguments","text":"<pre><code>--model resnet50             # Model architecture\n--checkpoint path.pth        # Path to checkpoint\n--pretrained                 # Use pretrained weights\n--num-classes 1000          # Number of classes\n--batch-size 256            # Validation batch size\n--workers 4                 # Number of workers\n--img-size 224              # Input image size\n--crop-pct 0.875            # Center crop percentage\n--interpolation bicubic     # Resize interpolation\n--amp                       # Use mixed precision\n--channels-last             # Use channels last format\n--results-file results.csv  # Save results to CSV\n</code></pre>"},{"location":"training/#test-time-augmentation","title":"Test-Time Augmentation","text":"<pre><code>python validate.py /imagenet \\\n  --model resnet50 \\\n  --checkpoint checkpoint.pth \\\n  --batch-size 64 \\\n  --img-size 288 \\  # Larger than training size\n  --crop-pct 1.0 \\  # Use full image\n  --amp\n</code></pre>"},{"location":"training/#inference-script","title":"Inference Script","text":""},{"location":"training/#basic-usage_2","title":"Basic Usage","text":"<pre><code>python inference.py /path/to/images \\\n  --model resnet50 \\\n  --checkpoint checkpoint.pth \\\n  --output predictions.csv\n</code></pre>"},{"location":"training/#batch-inference","title":"Batch Inference","text":"<pre><code>python inference.py /path/to/images \\\n  --model resnet50 \\\n  --pretrained \\\n  --batch-size 32 \\\n  --output predictions.csv \\\n  --topk 5  # Save top-5 predictions\n</code></pre>"},{"location":"training/#distributed-training","title":"Distributed Training","text":""},{"location":"training/#single-node-multiple-gpus","title":"Single Node, Multiple GPUs","text":"<pre><code>./distributed_train.sh 4 /imagenet \\\n  --model resnet50 \\\n  --batch-size 64 \\\n  --lr 0.1 \\\n  --epochs 100\n</code></pre> <p>The script automatically distributes across 4 GPUs (specified by the first argument).</p>"},{"location":"training/#multi-node-training","title":"Multi-Node Training","text":"<pre><code># Node 0\npython -m torch.distributed.launch \\\n  --nproc_per_node=4 \\\n  --nnodes=2 \\\n  --node_rank=0 \\\n  --master_addr=\"192.168.1.1\" \\\n  --master_port=12345 \\\n  train.py /imagenet --model resnet50 ...\n\n# Node 1\npython -m torch.distributed.launch \\\n  --nproc_per_node=4 \\\n  --nnodes=2 \\\n  --node_rank=1 \\\n  --master_addr=\"192.168.1.1\" \\\n  --master_port=12345 \\\n  train.py /imagenet --model resnet50 ...\n</code></pre>"},{"location":"training/#training-tips","title":"Training Tips","text":""},{"location":"training/#learning-rate-scaling","title":"Learning Rate Scaling","text":"<p>When using larger batch sizes, scale the learning rate:</p> <pre><code># Base: batch_size=128, lr=0.1\n# Scaled: batch_size=512, lr=0.4 (4x batch = 4x lr)\npython train.py /imagenet \\\n  --batch-size 512 \\\n  --lr 0.4\n</code></pre>"},{"location":"training/#gradient-accumulation","title":"Gradient Accumulation","text":"<p>For large models with limited memory:</p> <pre><code>python train.py /imagenet \\\n  --model vit_large_patch16_224 \\\n  --batch-size 32 \\  # Effective batch = 32 * 4 = 128\n  --grad-accum-steps 4 \\\n  --amp\n</code></pre>"},{"location":"training/#layer-wise-learning-rates","title":"Layer-wise Learning Rates","text":"<p>Different learning rates for different layers:</p> <pre><code>python train.py /imagenet \\\n  --model vit_base_patch16_224 \\\n  --lr 0.001 \\\n  --layer-decay 0.75  # Earlier layers use lower LR\n</code></pre>"},{"location":"training/#model-ema-best-practices","title":"Model EMA Best Practices","text":"<pre><code>python train.py /imagenet \\\n  --model-ema \\\n  --model-ema-decay 0.9998 \\  # For batch_size &gt;= 128\n  --model-ema-force-cpu  # Keep EMA on CPU to save GPU memory\n</code></pre>"},{"location":"training/#custom-datasets","title":"Custom Datasets","text":""},{"location":"training/#directory-structure","title":"Directory Structure","text":"<pre><code>dataset/\n\u251c\u2500\u2500 train/\n\u2502   \u251c\u2500\u2500 class1/\n\u2502   \u2502   \u251c\u2500\u2500 img1.jpg\n\u2502   \u2502   \u2514\u2500\u2500 img2.jpg\n\u2502   \u2514\u2500\u2500 class2/\n\u2502       \u251c\u2500\u2500 img1.jpg\n\u2502       \u2514\u2500\u2500 img2.jpg\n\u2514\u2500\u2500 val/\n    \u251c\u2500\u2500 class1/\n    \u2514\u2500\u2500 class2/\n</code></pre>"},{"location":"training/#training-on-custom-data","title":"Training on Custom Data","text":"<pre><code>python train.py /path/to/dataset \\\n  --dataset ImageFolder \\\n  --train-split train \\\n  --val-split val \\\n  --num-classes NUM_CLASSES \\\n  --model resnet50 \\\n  --pretrained\n</code></pre>"},{"location":"training/#monitoring-training","title":"Monitoring Training","text":""},{"location":"training/#tensorboard","title":"TensorBoard","text":"<pre><code>python train.py /imagenet \\\n  --log-interval 50 \\\n  --output /path/to/output\n\n# In another terminal\ntensorboard --logdir /path/to/output\n</code></pre>"},{"location":"training/#weights-biases","title":"Weights &amp; Biases","text":"<pre><code>python train.py /imagenet \\\n  --log-wandb \\\n  --wandb-project my-project\n</code></pre>"},{"location":"training/#checkpointing_1","title":"Checkpointing","text":""},{"location":"training/#save-checkpoints","title":"Save Checkpoints","text":"<p>Checkpoints are automatically saved to the output directory:</p> <pre><code>output/\n\u251c\u2500\u2500 checkpoint-0.pth.tar\n\u251c\u2500\u2500 checkpoint-10.pth.tar\n\u251c\u2500\u2500 checkpoint-20.pth.tar\n\u2514\u2500\u2500 model_best.pth.tar\n</code></pre>"},{"location":"training/#resume-training","title":"Resume Training","text":"<pre><code>python train.py /imagenet \\\n  --resume /path/to/checkpoint.pth.tar\n</code></pre>"},{"location":"training/#start-from-checkpoint-with-different-settings","title":"Start from Checkpoint with Different Settings","text":"<pre><code>python train.py /imagenet \\\n  --initial-checkpoint /path/to/checkpoint.pth.tar \\\n  --lr 0.0001  # New learning rate\n</code></pre>"},{"location":"training/#exporting-models","title":"Exporting Models","text":""},{"location":"training/#export-to-onnx","title":"Export to ONNX","text":"<pre><code>import torch\nimport timm\n\nmodel = timm.create_model('resnet50', pretrained=True)\nmodel.eval()\n\ndummy_input = torch.randn(1, 3, 224, 224)\n\ntorch.onnx.export(\n    model,\n    dummy_input,\n    \"resnet50.onnx\",\n    input_names=['input'],\n    output_names=['output'],\n    dynamic_axes={\n        'input': {0: 'batch_size'},\n        'output': {0: 'batch_size'}\n    }\n)\n</code></pre>"},{"location":"training/#export-to-torchscript","title":"Export to TorchScript","text":"<pre><code>model = timm.create_model('resnet50', pretrained=True)\nmodel.eval()\n\n# Trace\nexample = torch.randn(1, 3, 224, 224)\ntraced = torch.jit.trace(model, example)\ntraced.save('resnet50_traced.pt')\n\n# Script (for dynamic control flow)\nscripted = torch.jit.script(model)\nscripted.save('resnet50_scripted.pt')\n</code></pre>"},{"location":"training/#performance-optimization","title":"Performance Optimization","text":""},{"location":"training/#mixed-precision-training","title":"Mixed Precision Training","text":"<p>Significantly faster on modern GPUs:</p> <pre><code>--amp  # Enable automatic mixed precision\n</code></pre>"},{"location":"training/#channels-last-memory-format","title":"Channels Last Memory Format","text":"<p>Better performance on GPUs with Tensor Cores:</p> <pre><code>--channels-last\n</code></pre>"},{"location":"training/#fused-optimizers","title":"Fused Optimizers","text":"<p>Use APEX fused optimizers for better performance:</p> <pre><code>--opt fusedadam  # Requires NVIDIA APEX\n</code></pre>"},{"location":"training/#compile-with-torchdynamo-pytorch-20","title":"Compile with TorchDynamo (PyTorch 2.0+)","text":"<pre><code>--torchcompile  # Enable torch.compile()\n</code></pre>"},{"location":"training/#links","title":"Links","text":"<ul> <li>Getting Started</li> <li>Model Features</li> <li>Model Metrics</li> <li>Official Training Documentation</li> </ul>"},{"location":"metrics/2024-models/","title":"2024 Model Metrics","text":"<p>This page contains all models released or updated in 2024.</p>"},{"location":"metrics/2024-models/#october-2024-mambaout-models","title":"October 2024: MambaOut Models","text":"<p>MambaOut models - ConvNeXt-style architecture with gating, no SSM. Paper</p> Model Img Size Top-1 Top-5 Params (M) mambaout_base_plus_rw.sw_e150_r384_in12k_ft_in1k 384 87.506 98.428 101.66 mambaout_base_plus_rw.sw_e150_in12k_ft_in1k 288 86.912 98.236 101.66 mambaout_base_plus_rw.sw_e150_in12k_ft_in1k 224 86.632 98.156 101.66 mambaout_base_tall_rw.sw_e500_in1k 288 84.974 97.332 86.48 mambaout_base_wide_rw.sw_e500_in1k 288 84.962 97.208 94.45 mambaout_base_short_rw.sw_e500_in1k 288 84.832 97.270 88.83 mambaout_base.in1k 288 84.720 96.930 84.81 mambaout_small_rw.sw_e450_in1k 288 84.598 97.098 48.50 mambaout_small.in1k 288 84.500 96.974 48.49 mambaout_base_wide_rw.sw_e500_in1k 224 84.454 96.864 94.45 mambaout_base_tall_rw.sw_e500_in1k 224 84.434 96.958 86.48 mambaout_base_short_rw.sw_e500_in1k 224 84.362 96.952 88.83 mambaout_base.in1k 224 84.168 96.680 84.81 mambaout_small.in1k 224 84.086 96.630 48.49 mambaout_small_rw.sw_e450_in1k 224 84.024 96.752 48.50 mambaout_tiny.in1k 288 83.448 96.538 26.55 mambaout_tiny.in1k 224 82.736 96.100 26.55 mambaout_kobe.in1k 288 81.054 95.718 9.14 mambaout_kobe.in1k 224 79.986 94.986 9.14 mambaout_femto.in1k 288 79.848 95.140 7.30 mambaout_femto.in1k 224 78.870 94.408 7.30"},{"location":"metrics/2024-models/#siglip-so400m-vit-models","title":"SigLIP SO400M ViT Models","text":"Model Top-1 Img Size Params (M) vit_so400m_patch14_siglip_378.webli_ft_in1k 89.42 378 400+ vit_so400m_patch14_siglip_gap_378.webli_ft_in1k 89.03 378 400+ vit_so400m_patch14_siglip_256.webli (i18n) - 256 400+"},{"location":"metrics/2024-models/#convnext-zepto-models","title":"ConvNeXt Zepto Models","text":"<p>Ultra-small ConvNeXt models with RMSNorm (2.2M parameters):</p> Model Top-1 Img Size Params (M) convnext_zepto_rms_ols.ra4_e3600_r224_in1k 73.20 224 2.2 convnext_zepto_rms.ra4_e3600_r224_in1k 72.81 224 2.2"},{"location":"metrics/2024-models/#september-2024-tiny-test-models-mobilenet","title":"September 2024: Tiny Test Models &amp; MobileNet","text":""},{"location":"metrics/2024-models/#mobilenetv4-conv-small-05x","title":"MobileNetV4-Conv-Small (0.5x)","text":"Model Top-1 (256) Top-1 (224) Params (M) mobilenetv4_conv_small_050.e3000_r224_in1k 65.81 64.76 1.9"},{"location":"metrics/2024-models/#mobilenetv3-large-variants-mnv4-recipe","title":"MobileNetV3-Large Variants (MNV4 Recipe)","text":"Model Top-1 (320) Top-1 (256) Params (M) mobilenetv3_large_150d.ra4_e3600_r256_in1k 81.81 80.94 7.5 mobilenetv3_large_100.ra4_e3600_r224_in1k 77.16 76.31 5.5"},{"location":"metrics/2024-models/#tiny-test-models-05m-params","title":"Tiny Test Models (&lt; 0.5M params)","text":"<p>For testing and ultra-low resource applications:</p> Model Top-1 (192) Top-1 (160) Params (M) test_efficientnet.r160_in1k 47.156 46.426 0.36 test_byobnet.r160_in1k 46.698 45.378 0.46 test_vit.r160_in1k 42.000 40.822 0.37"},{"location":"metrics/2024-models/#august-2024-sbb-vit-updates-training-challenges","title":"August 2024: SBB ViT Updates &amp; Training Challenges","text":""},{"location":"metrics/2024-models/#updated-sbb-vit-models-imagenet-12k-imagenet-1k","title":"Updated SBB ViT Models (ImageNet-12k \u2192 ImageNet-1k)","text":"Model Top-1 Top-5 Params (M) Img Size vit_mediumd_patch16_reg4_gap_384.sbb2_e200_in12k_ft_in1k 87.438 98.256 64.11 384 vit_mediumd_patch16_reg4_gap_256.sbb2_e200_in12k_ft_in1k 86.608 97.934 64.11 256 vit_betwixt_patch16_reg4_gap_384.sbb2_e200_in12k_ft_in1k 86.594 98.020 60.40 384 vit_betwixt_patch16_reg4_gap_256.sbb2_e200_in12k_ft_in1k 85.734 97.610 60.40 256"},{"location":"metrics/2024-models/#mobilenet-v4-baseline-challenges","title":"MobileNet V4 Baseline Challenges","text":"<p>Training challenges for classic architectures with modern recipes:</p> Model Top-1 (288) Top-1 (224) Params (M) resnet50d.ra4_e3600_r224_in1k 81.838 80.952 25.58 efficientnet_b1.ra4_e3600_r240_in1k 81.440 80.406 7.79 mobilenetv1_125.ra4_e3600_r224_in1k 77.600 76.924 6.27"},{"location":"metrics/2024-models/#hiera-small-models","title":"Hiera Small Models","text":"Model Top-1 Top-5 Params (M) hiera_small_abswin_256.sbb2_e200_in12k_ft_in1k 84.912 97.260 35.01 hiera_small_abswin_256.sbb2_pd_e200_in12k_ft_in1k 84.560 97.106 35.01"},{"location":"metrics/2024-models/#july-2024-mobilenet-v4-baseline-models","title":"July 2024: MobileNet-V4 &amp; Baseline Models","text":""},{"location":"metrics/2024-models/#mobilenetv4-models-imagenet-12k-pretrain","title":"MobileNetV4 Models (ImageNet-12k Pretrain)","text":"Model Top-1 Top-5 Params (M) Img Size mobilenetv4_conv_aa_large.e230_r448_in12k_ft_in1k 84.990 97.294 32.59 544 mobilenetv4_conv_aa_large.e230_r384_in12k_ft_in1k 84.772 97.344 32.59 480 mobilenetv4_conv_aa_large.e230_r448_in12k_ft_in1k 84.640 97.114 32.59 448 mobilenetv4_conv_aa_large.e230_r384_in12k_ft_in1k 84.314 97.102 32.59 384 mobilenetv4_hybrid_medium.e200_r256_in12k_ft_in1k 82.990 96.670 11.07 320 mobilenetv4_hybrid_medium.e200_r256_in12k_ft_in1k 82.364 96.256 11.07 256"},{"location":"metrics/2024-models/#mobilenet-v1-efficientnet-b0-baseline-challenges","title":"MobileNet-V1 &amp; EfficientNet-B0 Baseline Challenges","text":"<p>Impressive results with modern training recipes:</p> Model Top-1 (256) Top-1 (224) Params (M) efficientnet_b0.ra4_e3600_r224_in1k 79.364 78.584 5.29 mobilenetv1_100h.ra4_e3600_r224_in1k 76.596 75.662 5.28 mobilenetv1_100.ra4_e3600_r224_in1k 76.094 75.382 4.23"},{"location":"metrics/2024-models/#mobileedge-tpu-v2","title":"MobileEdge TPU V2","text":"Model Top-1 (256) Top-1 (224) Params (M) mobilenet_edgetpu_v2_m.ra4_e3600_r224_in1k 80.700 80.100 6.9"},{"location":"metrics/2024-models/#june-2024-mobilenetv4-initial-release","title":"June 2024: MobileNetV4 Initial Release","text":""},{"location":"metrics/2024-models/#mobilenetv4-hybrid-models","title":"MobileNetV4 Hybrid Models","text":"Model Top-1 Top-5 Params (M) Img Size mobilenetv4_hybrid_large.ix_e600_r384_in1k 84.356 96.892 37.76 448 mobilenetv4_hybrid_large.ix_e600_r384_in1k 83.990 96.702 37.76 384 mobilenetv4_hybrid_large.e600_r384_in1k 84.266 96.936 37.76 448 mobilenetv4_hybrid_large.e600_r384_in1k 83.800 96.770 37.76 384 mobilenetv4_hybrid_medium.ix_e550_r384_in1k 83.394 96.760 11.07 448 mobilenetv4_hybrid_medium.ix_e550_r384_in1k 82.968 96.474 11.07 384 mobilenetv4_hybrid_medium.ix_e550_r256_in1k 82.492 96.278 11.07 320 mobilenetv4_hybrid_medium.ix_e550_r256_in1k 81.446 95.704 11.07 256 mobilenetv4_hybrid_medium.e500_r224_in1k 81.276 95.742 11.07 256 mobilenetv4_hybrid_medium.e500_r224_in1k 80.442 95.380 11.07 224"},{"location":"metrics/2024-models/#mobilenetv4-conv-models","title":"MobileNetV4 Conv Models","text":"Model Top-1 Top-5 Params (M) Img Size mobilenetv4_conv_large.e600_r384_in1k 83.392 96.622 32.59 448 mobilenetv4_conv_large.e600_r384_in1k 82.952 96.266 32.59 384 mobilenetv4_conv_large.e500_r256_in1k 82.674 96.310 32.59 320 mobilenetv4_conv_large.e500_r256_in1k 81.862 95.690 32.59 256 mobilenetv4_conv_aa_large.e600_r384_in1k 83.824 96.734 32.59 480 mobilenetv4_conv_aa_large.e600_r384_in1k 83.244 96.392 32.59 384 mobilenetv4_conv_medium.e500_r256_in1k 80.858 95.768 9.72 320 mobilenetv4_conv_medium.e500_r256_in1k 79.928 95.184 9.72 256 mobilenetv4_conv_medium.e500_r224_in1k 79.808 95.186 9.72 256 mobilenetv4_conv_medium.e500_r224_in1k 79.094 94.770 9.72 224 mobilenetv4_conv_blur_medium.e500_r224_in1k 80.142 95.298 9.72 256 mobilenetv4_conv_blur_medium.e500_r224_in1k 79.438 94.932 9.72 224 mobilenetv4_conv_small.e2400_r224_in1k 74.616 92.072 3.77 256 mobilenetv4_conv_small.e2400_r224_in1k 73.756 91.422 3.77 224 mobilenetv4_conv_small.e1200_r224_in1k 74.292 92.116 3.77 256 mobilenetv4_conv_small.e1200_r224_in1k 73.454 91.340 3.77 224"},{"location":"metrics/2024-models/#model-naming-convention","title":"Model Naming Convention","text":"<ul> <li>ra4: RandAugment training recipe with magnitude 4</li> <li>sw: StochDepth + weight decay variant</li> <li>e: Number of training epochs <li>r: Training resolution <li>in1k: ImageNet-1k dataset</li> <li>in12k: ImageNet-12k dataset</li> <li>ft: Fine-tuned</li> <li>aa: Anti-aliased downsampling</li> <li>ix: Improved attention initialization</li> <li>rw: Ross Wightman (timm author) trained weights</li>"},{"location":"metrics/2024-models/#links","title":"Links","text":"<ul> <li>Back to Overview</li> <li>2025 Models</li> <li>Historical Models</li> </ul>"},{"location":"metrics/2025-models/","title":"2025 Model Metrics","text":"<p>This page contains all models released or updated in 2025.</p>"},{"location":"metrics/2025-models/#july-2025-naver-rope-vit-models","title":"July 2025: Naver ROPE-ViT Models","text":"<p>ROPE-ViT models from Naver with rotary position embeddings. Paper</p> Model Img Size Top-1 Top-5 Params (M) vit_large_patch16_rope_mixed_ape_224.naver_in1k 224 84.840 97.122 304.40 vit_large_patch16_rope_mixed_224.naver_in1k 224 84.828 97.116 304.20 vit_large_patch16_rope_ape_224.naver_in1k 224 84.650 97.154 304.37 vit_large_patch16_rope_224.naver_in1k 224 84.648 97.122 304.17 vit_base_patch16_rope_mixed_ape_224.naver_in1k 224 83.894 96.754 86.59 vit_base_patch16_rope_mixed_224.naver_in1k 224 83.804 96.712 86.44 vit_base_patch16_rope_ape_224.naver_in1k 224 83.782 96.610 86.59 vit_base_patch16_rope_224.naver_in1k 224 83.718 96.672 86.43 vit_small_patch16_rope_224.naver_in1k 224 81.230 95.022 21.98 vit_small_patch16_rope_mixed_224.naver_in1k 224 81.216 95.022 21.99 vit_small_patch16_rope_ape_224.naver_in1k 224 81.004 95.016 22.06 vit_small_patch16_rope_mixed_ape_224.naver_in1k 224 80.986 94.976 22.06"},{"location":"metrics/2025-models/#june-2025-naflexvit-models","title":"June 2025: NaFlexViT Models","text":"<p>Initial NaFlexViT checkpoints with native aspect ratio support.</p> Model Top-1 Top-5 Params (M) Eval Seq Len naflexvit_base_patch16_par_gap.e300_s576_in1k 83.67 96.45 86.63 576 naflexvit_base_patch16_parfac_gap.e300_s576_in1k 83.63 96.41 86.46 576 naflexvit_base_patch16_gap.e300_s576_in1k 83.50 96.46 86.63 576"},{"location":"metrics/2025-models/#may-2025-searching-for-better-vit-baselines","title":"May 2025: Searching for Better ViT Baselines","text":"<p>Exploring model shapes between Tiny and Base with SBB training recipes.</p>"},{"location":"metrics/2025-models/#imagenet-12k-pretrain-imagenet-1k-fine-tune","title":"ImageNet-12k Pretrain + ImageNet-1k Fine-tune","text":"Model Top-1 Top-5 Params (M) Img Size vit_mediumd_patch16_reg4_gap_256.sbb_in12k_ft_in1k 86.202 97.874 64.11 256 vit_betwixt_patch16_reg4_gap_256.sbb_in12k_ft_in1k 85.418 97.480 60.40 256"},{"location":"metrics/2025-models/#imagenet-1k-only-training","title":"ImageNet-1k Only Training","text":"Model Top-1 Top-5 Params (M) Img Size vit_mediumd_patch16_rope_reg1_gap_256.sbb_in1k 84.322 96.812 63.95 256 vit_betwixt_patch16_rope_reg4_gap_256.sbb_in1k 83.906 96.684 60.23 256 vit_base_patch16_rope_reg1_gap_256.sbb_in1k 83.866 96.670 86.43 256 vit_medium_patch16_rope_reg1_gap_256.sbb_in1k 83.810 96.824 38.74 256 vit_betwixt_patch16_reg4_gap_256.sbb_in1k 83.706 96.616 60.40 256 vit_betwixt_patch16_reg1_gap_256.sbb_in1k 83.628 96.544 60.40 256 vit_medium_patch16_reg4_gap_256.sbb_in1k 83.470 96.622 38.88 256 vit_medium_patch16_reg1_gap_256.sbb_in1k 83.462 96.548 38.88 256 vit_little_patch16_reg4_gap_256.sbb_in1k 82.514 96.262 22.52 256 vit_wee_patch16_reg1_gap_256.sbb_in1k 80.256 95.360 13.42 256 vit_pwee_patch16_reg1_gap_256.sbb_in1k 80.072 95.136 15.25 256"},{"location":"metrics/2025-models/#february-2025-siglip-2-so150m2-models","title":"February 2025: SigLIP 2 &amp; SO150M2 Models","text":""},{"location":"metrics/2025-models/#siglip-2-vit-image-encoders","title":"SigLIP 2 ViT Image Encoders","text":"<p>Variable resolution / aspect NaFlex versions available.</p>"},{"location":"metrics/2025-models/#so150m2-vit-models","title":"SO150M2 ViT Models","text":"<p>SBB-trained models with excellent ImageNet results:</p> Model Top-1 Img Size Params (M) vit_so150m2_patch16_reg1_gap_448.sbb_e200_in12k_ft_in1k 88.1 448 150+ vit_so150m2_patch16_reg1_gap_384.sbb_e200_in12k_ft_in1k 87.9 384 150+ vit_so150m2_patch16_reg1_gap_256.sbb_e200_in12k_ft_in1k 87.3 256 150+ vit_so150m2_patch16_reg4_gap_256.sbb_e200_in12k - 256 150+"},{"location":"metrics/2025-models/#january-2025-so150m-vit-models","title":"January 2025: SO150M ViT Models","text":"Model Top-1 Img Size Params (M) vit_so150m_patch16_reg4_gap_384.sbb_e250_in12k_ft_in1k 87.4 384 150+ vit_so150m_patch16_reg4_gap_256.sbb_e250_in12k_ft_in1k 86.7 256 150+ vit_so150m_patch16_reg4_gap_256.sbb_e250_in12k - 256 150+"},{"location":"metrics/2025-models/#model-naming-convention","title":"Model Naming Convention","text":"<p>Understanding the model names:</p> <ul> <li>vit: Vision Transformer base architecture</li> <li>patch16: 16x16 patch size</li> <li>rope: Rotary Position Embedding</li> <li>reg1/reg4: Number of register tokens</li> <li>gap: Global Average Pooling</li> <li>256/384/448: Input image resolution</li> <li>sbb: Searching for Better Baselines training recipe</li> <li>e200/e250: Number of training epochs</li> <li>in12k: Pre-trained on ImageNet-12k</li> <li>ft_in1k: Fine-tuned on ImageNet-1k</li> <li>naver_in1k: Trained by Naver on ImageNet-1k</li> </ul>"},{"location":"metrics/2025-models/#training-details","title":"Training Details","text":""},{"location":"metrics/2025-models/#sbb-recipe","title":"SBB Recipe","text":"<p>The \"Searching for Better ViT Baselines\" recipe focuses on: - Efficient training for GPU-constrained environments - Strong regularization with register tokens - Optimal hyperparameters for medium-sized models</p>"},{"location":"metrics/2025-models/#rope-variants","title":"ROPE Variants","text":"<p>Rotary Position Embedding (ROPE) variants include: - rope: Standard ROPE - rope_mixed: Mixed ROPE mode - ape: Absolute Position Embedding addition - mixed_ape: Combined mixed ROPE with APE</p>"},{"location":"metrics/2025-models/#links","title":"Links","text":"<ul> <li>Back to Overview</li> <li>2024 Models</li> <li>Historical Models</li> </ul>"},{"location":"metrics/all-models/","title":"Complete Model Benchmark Results","text":"<p>This page contains a comprehensive table of all model benchmarks extracted from timm. Use your browser's search (Ctrl+F / Cmd+F) or the table sorting to find specific models.</p> <p>How to Use This Table</p> <ul> <li>Click column headers to sort by that metric</li> <li>Use browser search (Ctrl+F / Cmd+F) to find specific models</li> <li>Top-1 and Top-5 are ImageNet-1k validation accuracies (%)</li> <li>Params are in millions (M)</li> <li>Image Size is the input resolution used for validation</li> </ul>"},{"location":"metrics/all-models/#complete-benchmark-table","title":"Complete Benchmark Table","text":""},{"location":"metrics/all-models/#2025-models","title":"2025 Models","text":""},{"location":"metrics/all-models/#naver-rope-vit-models-july-2025","title":"Naver ROPE-ViT Models (July 2025)","text":"Model Year Top-1 Top-5 Params (M) Img Size Notes vit_large_patch16_rope_mixed_ape_224.naver_in1k 2025 84.840 97.122 304.40 224 ROPE + Mixed + APE vit_large_patch16_rope_mixed_224.naver_in1k 2025 84.828 97.116 304.20 224 ROPE + Mixed vit_large_patch16_rope_ape_224.naver_in1k 2025 84.650 97.154 304.37 224 ROPE + APE vit_large_patch16_rope_224.naver_in1k 2025 84.648 97.122 304.17 224 ROPE vit_base_patch16_rope_mixed_ape_224.naver_in1k 2025 83.894 96.754 86.59 224 ROPE + Mixed + APE vit_base_patch16_rope_mixed_224.naver_in1k 2025 83.804 96.712 86.44 224 ROPE + Mixed vit_base_patch16_rope_ape_224.naver_in1k 2025 83.782 96.610 86.59 224 ROPE + APE vit_base_patch16_rope_224.naver_in1k 2025 83.718 96.672 86.43 224 ROPE vit_small_patch16_rope_224.naver_in1k 2025 81.230 95.022 21.98 224 ROPE vit_small_patch16_rope_mixed_224.naver_in1k 2025 81.216 95.022 21.99 224 ROPE + Mixed vit_small_patch16_rope_ape_224.naver_in1k 2025 81.004 95.016 22.06 224 ROPE + APE vit_small_patch16_rope_mixed_ape_224.naver_in1k 2025 80.986 94.976 22.06 224 ROPE + Mixed + APE"},{"location":"metrics/all-models/#naflexvit-models-june-2025","title":"NaFlexViT Models (June 2025)","text":"Model Year Top-1 Top-5 Params (M) Img Size Eval Seq Len Notes naflexvit_base_patch16_par_gap.e300_s576_in1k 2025 83.67 96.45 86.63 224 576 Native aspect naflexvit_base_patch16_parfac_gap.e300_s576_in1k 2025 83.63 96.41 86.46 224 576 Factorized pos embed naflexvit_base_patch16_gap.e300_s576_in1k 2025 83.50 96.46 86.63 224 576 Native aspect"},{"location":"metrics/all-models/#sbb-vit-models-may-2025","title":"SBB ViT Models (May 2025)","text":"Model Year Top-1 Top-5 Params (M) Img Size Notes vit_mediumd_patch16_reg4_gap_256.sbb_in12k_ft_in1k 2025 86.202 97.874 64.11 256 IN12k pretrain vit_betwixt_patch16_reg4_gap_256.sbb_in12k_ft_in1k 2025 85.418 97.480 60.40 256 IN12k pretrain vit_mediumd_patch16_rope_reg1_gap_256.sbb_in1k 2025 84.322 96.812 63.95 256 ROPE vit_betwixt_patch16_rope_reg4_gap_256.sbb_in1k 2025 83.906 96.684 60.23 256 ROPE vit_base_patch16_rope_reg1_gap_256.sbb_in1k 2025 83.866 96.670 86.43 256 ROPE vit_medium_patch16_rope_reg1_gap_256.sbb_in1k 2025 83.810 96.824 38.74 256 ROPE vit_betwixt_patch16_reg4_gap_256.sbb_in1k 2025 83.706 96.616 60.40 256 SBB recipe vit_betwixt_patch16_reg1_gap_256.sbb_in1k 2025 83.628 96.544 60.40 256 SBB recipe vit_medium_patch16_reg4_gap_256.sbb_in1k 2025 83.470 96.622 38.88 256 SBB recipe vit_medium_patch16_reg1_gap_256.sbb_in1k 2025 83.462 96.548 38.88 256 SBB recipe vit_little_patch16_reg4_gap_256.sbb_in1k 2025 82.514 96.262 22.52 256 SBB recipe vit_wee_patch16_reg1_gap_256.sbb_in1k 2025 80.256 95.360 13.42 256 SBB recipe vit_pwee_patch16_reg1_gap_256.sbb_in1k 2025 80.072 95.136 15.25 256 SBB recipe"},{"location":"metrics/all-models/#so150m2-vit-models-february-2025","title":"SO150M2 ViT Models (February 2025)","text":"Model Year Top-1 Top-5 Params (M) Img Size Notes vit_so150m2_patch16_reg1_gap_448.sbb_e200_in12k_ft_in1k 2025 88.1 - 150+ 448 IN12k pretrain vit_so150m2_patch16_reg1_gap_384.sbb_e200_in12k_ft_in1k 2025 87.9 - 150+ 384 IN12k pretrain vit_so150m2_patch16_reg1_gap_256.sbb_e200_in12k_ft_in1k 2025 87.3 - 150+ 256 IN12k pretrain"},{"location":"metrics/all-models/#so150m-vit-models-january-2025","title":"SO150M ViT Models (January 2025)","text":"Model Year Top-1 Top-5 Params (M) Img Size Notes vit_so150m_patch16_reg4_gap_384.sbb_e250_in12k_ft_in1k 2025 87.4 - 150+ 384 IN12k pretrain vit_so150m_patch16_reg4_gap_256.sbb_e250_in12k_ft_in1k 2025 86.7 - 150+ 256 IN12k pretrain"},{"location":"metrics/all-models/#2024-models","title":"2024 Models","text":""},{"location":"metrics/all-models/#mambaout-models-october-2024","title":"MambaOut Models (October 2024)","text":"Model Year Top-1 Top-5 Params (M) Img Size Notes mambaout_base_plus_rw.sw_e150_r384_in12k_ft_in1k 2024 87.506 98.428 101.66 384 IN12k pretrain mambaout_base_plus_rw.sw_e150_in12k_ft_in1k 2024 86.912 98.236 101.66 288 IN12k pretrain mambaout_base_plus_rw.sw_e150_in12k_ft_in1k 2024 86.632 98.156 101.66 224 IN12k pretrain mambaout_base_tall_rw.sw_e500_in1k 2024 84.974 97.332 86.48 288 Tall variant mambaout_base_wide_rw.sw_e500_in1k 2024 84.962 97.208 94.45 288 Wide variant mambaout_base_short_rw.sw_e500_in1k 2024 84.832 97.270 88.83 288 Short variant mambaout_base.in1k 2024 84.720 96.930 84.81 288 Base mambaout_small_rw.sw_e450_in1k 2024 84.598 97.098 48.50 288 Small variant mambaout_small.in1k 2024 84.500 96.974 48.49 288 Small mambaout_base_wide_rw.sw_e500_in1k 2024 84.454 96.864 94.45 224 Wide variant mambaout_base_tall_rw.sw_e500_in1k 2024 84.434 96.958 86.48 224 Tall variant mambaout_base_short_rw.sw_e500_in1k 2024 84.362 96.952 88.83 224 Short variant mambaout_base.in1k 2024 84.168 96.680 84.81 224 Base mambaout_small.in1k 2024 84.086 96.630 48.49 224 Small mambaout_small_rw.sw_e450_in1k 2024 84.024 96.752 48.50 224 Small variant mambaout_tiny.in1k 2024 83.448 96.538 26.55 288 Tiny mambaout_tiny.in1k 2024 82.736 96.100 26.55 224 Tiny mambaout_kobe.in1k 2024 81.054 95.718 9.14 288 Ultra-small mambaout_kobe.in1k 2024 79.986 94.986 9.14 224 Ultra-small mambaout_femto.in1k 2024 79.848 95.140 7.30 288 Ultra-small mambaout_femto.in1k 2024 78.870 94.408 7.30 224 Ultra-small"},{"location":"metrics/all-models/#siglip-so400m-models-october-2024","title":"SigLIP SO400M Models (October 2024)","text":"Model Year Top-1 Top-5 Params (M) Img Size Notes vit_so400m_patch14_siglip_378.webli_ft_in1k 2024 89.42 - 400+ 378 WebLI pretrain vit_so400m_patch14_siglip_gap_378.webli_ft_in1k 2024 89.03 - 400+ 378 WebLI pretrain + GAP"},{"location":"metrics/all-models/#convnext-zepto-models-october-2024","title":"ConvNeXt Zepto Models (October 2024)","text":"Model Year Top-1 Top-5 Params (M) Img Size Notes convnext_zepto_rms_ols.ra4_e3600_r224_in1k 2024 73.20 - 2.2 224 Overlapped stem convnext_zepto_rms.ra4_e3600_r224_in1k 2024 72.81 - 2.2 224 Patch stem"},{"location":"metrics/all-models/#updated-sbb-vit-models-august-2024","title":"Updated SBB ViT Models (August 2024)","text":"Model Year Top-1 Top-5 Params (M) Img Size Notes vit_mediumd_patch16_reg4_gap_384.sbb2_e200_in12k_ft_in1k 2024 87.438 98.256 64.11 384 IN12k pretrain vit_mediumd_patch16_reg4_gap_256.sbb2_e200_in12k_ft_in1k 2024 86.608 97.934 64.11 256 IN12k pretrain vit_betwixt_patch16_reg4_gap_384.sbb2_e200_in12k_ft_in1k 2024 86.594 98.020 60.40 384 IN12k pretrain vit_betwixt_patch16_reg4_gap_256.sbb2_e200_in12k_ft_in1k 2024 85.734 97.610 60.40 256 IN12k pretrain"},{"location":"metrics/all-models/#baseline-challenge-models-august-2024","title":"Baseline Challenge Models (August 2024)","text":"Model Year Top-1 Top-5 Params (M) Img Size Notes resnet50d.ra4_e3600_r224_in1k 2024 81.838 95.922 25.58 288 RA4 recipe resnet50d.ra4_e3600_r224_in1k 2024 80.952 95.384 25.58 224 RA4 recipe efficientnet_b1.ra4_e3600_r240_in1k 2024 81.440 95.700 7.79 288 RA4 recipe efficientnet_b1.ra4_e3600_r240_in1k 2024 80.406 95.152 7.79 240 RA4 recipe mobilenetv1_125.ra4_e3600_r224_in1k 2024 77.600 93.804 6.27 256 RA4 recipe mobilenetv1_125.ra4_e3600_r224_in1k 2024 76.924 93.234 6.27 224 RA4 recipe"},{"location":"metrics/all-models/#hiera-models-august-2024","title":"Hiera Models (August 2024)","text":"Model Year Top-1 Top-5 Params (M) Img Size Notes hiera_small_abswin_256.sbb2_e200_in12k_ft_in1k 2024 84.912 97.260 35.01 256 IN12k pretrain hiera_small_abswin_256.sbb2_pd_e200_in12k_ft_in1k 2024 84.560 97.106 35.01 256 IN12k pretrain + PD"},{"location":"metrics/all-models/#mobilenetv4-models-june-july-2024","title":"MobileNetV4 Models (June-July 2024)","text":"Model Year Top-1 Top-5 Params (M) Img Size Notes mobilenetv4_conv_aa_large.e230_r448_in12k_ft_in1k 2024 84.990 97.294 32.59 544 Anti-aliased + IN12k mobilenetv4_conv_aa_large.e230_r384_in12k_ft_in1k 2024 84.772 97.344 32.59 480 Anti-aliased + IN12k mobilenetv4_conv_aa_large.e230_r448_in12k_ft_in1k 2024 84.640 97.114 32.59 448 Anti-aliased + IN12k mobilenetv4_conv_aa_large.e230_r384_in12k_ft_in1k 2024 84.314 97.102 32.59 384 Anti-aliased + IN12k mobilenetv4_hybrid_large.ix_e600_r384_in1k 2024 84.356 96.892 37.76 448 Hybrid + IX init mobilenetv4_hybrid_large.ix_e600_r384_in1k 2024 83.990 96.702 37.76 384 Hybrid + IX init mobilenetv4_conv_aa_large.e600_r384_in1k 2024 83.824 96.734 32.59 480 Anti-aliased mobilenetv4_hybrid_medium.ix_e550_r384_in1k 2024 83.394 96.760 11.07 448 Hybrid + IX init mobilenetv4_conv_large.e600_r384_in1k 2024 83.392 96.622 32.59 448 Conv variant mobilenetv4_conv_aa_large.e600_r384_in1k 2024 83.244 96.392 32.59 384 Anti-aliased mobilenetv4_hybrid_medium.ix_e550_r384_in1k 2024 82.968 96.474 11.07 384 Hybrid + IX init mobilenetv4_conv_large.e600_r384_in1k 2024 82.952 96.266 32.59 384 Conv variant mobilenetv4_hybrid_medium.e200_r256_in12k_ft_in1k 2024 82.990 96.670 11.07 320 IN12k pretrain mobilenetv4_conv_large.e500_r256_in1k 2024 82.674 96.310 32.59 320 Conv variant mobilenetv4_hybrid_medium.ix_e550_r256_in1k 2024 82.492 96.278 11.07 320 Hybrid + IX init mobilenetv4_hybrid_medium.e200_r256_in12k_ft_in1k 2024 82.364 96.256 11.07 256 IN12k pretrain mobilenetv4_conv_large.e500_r256_in1k 2024 81.862 95.690 32.59 256 Conv variant mobilenetv4_hybrid_medium.ix_e550_r256_in1k 2024 81.446 95.704 11.07 256 Hybrid + IX init mobilenetv4_hybrid_medium.e500_r224_in1k 2024 81.276 95.742 11.07 256 Hybrid mobilenetv4_conv_medium.e500_r256_in1k 2024 80.858 95.768 9.72 320 Conv medium mobilenetv4_hybrid_medium.e500_r224_in1k 2024 80.442 95.380 11.07 224 Hybrid mobilenetv4_conv_blur_medium.e500_r224_in1k 2024 80.142 95.298 9.72 256 Blur pooling mobilenetv4_conv_medium.e500_r256_in1k 2024 79.928 95.184 9.72 256 Conv medium mobilenetv4_conv_medium.e500_r224_in1k 2024 79.808 95.186 9.72 256 Conv medium mobilenetv4_conv_blur_medium.e500_r224_in1k 2024 79.438 94.932 9.72 224 Blur pooling mobilenetv4_conv_medium.e500_r224_in1k 2024 79.094 94.770 9.72 224 Conv medium mobilenet_edgetpu_v2_m.ra4_e3600_r224_in1k 2024 80.700 - 6.9 256 EdgeTPU optimized mobilenet_edgetpu_v2_m.ra4_e3600_r224_in1k 2024 80.100 - 6.9 224 EdgeTPU optimized mobilenetv4_conv_small.e2400_r224_in1k 2024 74.616 92.072 3.77 256 Conv small mobilenetv4_conv_small.e1200_r224_in1k 2024 74.292 92.116 3.77 256 Conv small mobilenetv4_conv_small.e2400_r224_in1k 2024 73.756 91.422 3.77 224 Conv small mobilenetv4_conv_small.e1200_r224_in1k 2024 73.454 91.340 3.77 224 Conv small"},{"location":"metrics/all-models/#mobilenet-baseline-models-july-2024","title":"MobileNet Baseline Models (July 2024)","text":"Model Year Top-1 Top-5 Params (M) Img Size Notes efficientnet_b0.ra4_e3600_r224_in1k 2024 79.364 94.754 5.29 256 RA4 recipe efficientnet_b0.ra4_e3600_r224_in1k 2024 78.584 94.338 5.29 224 RA4 recipe mobilenetv1_100h.ra4_e3600_r224_in1k 2024 76.596 93.272 5.28 256 RA4 recipe mobilenetv1_100.ra4_e3600_r224_in1k 2024 76.094 93.004 4.23 256 RA4 recipe mobilenetv1_100h.ra4_e3600_r224_in1k 2024 75.662 92.504 5.28 224 RA4 recipe mobilenetv1_100.ra4_e3600_r224_in1k 2024 75.382 92.312 4.23 224 RA4 recipe"},{"location":"metrics/all-models/#mobilenetv3-small-models-september-2024","title":"MobileNetV3 &amp; Small Models (September 2024)","text":"Model Year Top-1 Top-5 Params (M) Img Size Notes mobilenetv3_large_150d.ra4_e3600_r256_in1k 2024 81.81 - 7.5 320 RA4 recipe mobilenetv3_large_150d.ra4_e3600_r256_in1k 2024 80.94 - 7.5 256 RA4 recipe mobilenetv3_large_100.ra4_e3600_r224_in1k 2024 77.16 - 5.5 256 RA4 recipe mobilenetv3_large_100.ra4_e3600_r224_in1k 2024 76.31 - 5.5 224 RA4 recipe mobilenetv4_conv_small_050.e3000_r224_in1k 2024 65.81 - 1.9 256 0.5x width mobilenetv4_conv_small_050.e3000_r224_in1k 2024 64.76 - 1.9 224 0.5x width"},{"location":"metrics/all-models/#tiny-test-models-september-2024","title":"Tiny Test Models (September 2024)","text":"Model Year Top-1 Top-5 Params (M) Img Size Notes test_efficientnet.r160_in1k 2024 47.156 71.726 0.36 192 Test model test_byobnet.r160_in1k 2024 46.698 71.674 0.46 192 Test model test_efficientnet.r160_in1k 2024 46.426 70.928 0.36 160 Test model test_byobnet.r160_in1k 2024 45.378 70.572 0.46 160 Test model test_vit.r160_in1k 2024 42.000 68.664 0.37 192 Test model test_vit.r160_in1k 2024 40.822 67.212 0.37 160 Test model"},{"location":"metrics/all-models/#summary-statistics","title":"Summary Statistics","text":""},{"location":"metrics/all-models/#top-10-models-by-accuracy-top-1","title":"Top-10 Models by Accuracy (Top-1)","text":"<ol> <li>vit_so400m_patch14_siglip_378.webli_ft_in1k: 89.42% (400M params, 378px)</li> <li>vit_so400m_patch14_siglip_gap_378.webli_ft_in1k: 89.03% (400M params, 378px)</li> <li>vit_so150m2_patch16_reg1_gap_448.sbb_e200_in12k_ft_in1k: 88.1% (150M params, 448px)</li> <li>vit_so150m2_patch16_reg1_gap_384.sbb_e200_in12k_ft_in1k: 87.9% (150M params, 384px)</li> <li>mambaout_base_plus_rw.sw_e150_r384_in12k_ft_in1k: 87.506% (101.66M params, 384px)</li> <li>vit_mediumd_patch16_reg4_gap_384.sbb2_e200_in12k_ft_in1k: 87.438% (64.11M params, 384px)</li> <li>vit_so150m_patch16_reg4_gap_384.sbb_e250_in12k_ft_in1k: 87.4% (150M params, 384px)</li> <li>vit_so150m2_patch16_reg1_gap_256.sbb_e200_in12k_ft_in1k: 87.3% (150M params, 256px)</li> <li>mambaout_base_plus_rw.sw_e150_in12k_ft_in1k: 86.912% (101.66M params, 288px)</li> <li>vit_so150m_patch16_reg4_gap_256.sbb_e250_in12k_ft_in1k: 86.7% (150M params, 256px)</li> </ol>"},{"location":"metrics/all-models/#most-efficient-models-params-10m","title":"Most Efficient Models (Params &lt; 10M)","text":"<ol> <li>mambaout_kobe.in1k: 81.054% with 9.14M params @ 288px</li> <li>mobilenetv4_conv_medium: 80.858% with 9.72M params @ 320px</li> <li>mambaout_kobe.in1k: 79.986% with 9.14M params @ 224px</li> <li>mambaout_femto.in1k: 79.848% with 7.30M params @ 288px</li> <li>efficientnet_b0.ra4_e3600_r224_in1k: 79.364% with 5.29M params @ 256px</li> </ol>"},{"location":"metrics/all-models/#architecture-distribution","title":"Architecture Distribution","text":"<ul> <li>Vision Transformers (ViT): 40+ variants</li> <li>MambaOut: 21 variants</li> <li>MobileNetV4: 30+ variants</li> <li>ConvNeXt: 2 variants (Zepto)</li> <li>Hiera: 2 variants</li> <li>ResNet: 2 variants with modern training</li> <li>EfficientNet: 3 variants with modern training</li> <li>MobileNet V1/V3: 8 variants</li> </ul>"},{"location":"metrics/all-models/#notes","title":"Notes","text":"<ul> <li>All metrics are on ImageNet-1k validation set unless otherwise specified</li> <li>IN12k pretrain: Model was pre-trained on ImageNet-12k before fine-tuning on ImageNet-1k</li> <li>WebLI: Pre-trained on large-scale web images</li> <li>RA4: RandAugment with magnitude 4 training recipe</li> <li>SBB: \"Searching for Better ViT Baselines\" training recipe</li> <li>ROPE: Rotary Position Embedding</li> <li>GAP: Global Average Pooling</li> <li>Anti-aliased: Uses blur pooling for better shift-invariance</li> </ul>"},{"location":"metrics/all-models/#links","title":"Links","text":"<ul> <li>Back to Metrics Overview</li> <li>2025 Models Details</li> <li>2024 Models Details</li> <li>Historical Models</li> </ul>"},{"location":"metrics/historical/","title":"Historical Model Metrics","text":"<p>This page contains models and metrics from earlier releases (pre-2024).</p> <p>Historical Archive</p> <p>This page archives model performance metrics from the early development of timm. Many of these models remain highly relevant and widely used in production today.</p>"},{"location":"metrics/historical/#classic-vision-transformers","title":"Classic Vision Transformers","text":"<p>The original Vision Transformer implementations that established the baseline for transformer-based vision models.</p>"},{"location":"metrics/historical/#standard-vit-models","title":"Standard ViT Models","text":"Model Top-1 Top-5 Params (M) Img Size vit_base_patch16_224 81.8 95.6 86.6 224 vit_base_patch16_384 83.1 96.5 86.6 384 vit_large_patch16_224 82.6 96.1 304.3 224 vit_large_patch16_384 83.8 96.8 304.3 384"},{"location":"metrics/historical/#resnet-family","title":"ResNet Family","text":"<p>The foundational convolutional architectures that dominated computer vision before transformers.</p>"},{"location":"metrics/historical/#standard-resnet","title":"Standard ResNet","text":"Model Top-1 Top-5 Params (M) Img Size resnet18 69.8 89.1 11.7 224 resnet34 73.3 91.4 21.8 224 resnet50 76.1 92.9 25.6 224 resnet101 77.4 93.5 44.5 224 resnet152 78.3 94.1 60.2 224"},{"location":"metrics/historical/#resnet-d-variants","title":"ResNet-D Variants","text":"<p>Improved stem and downsampling:</p> Model Top-1 Params (M) Img Size resnet50d 77.6 25.6 224 resnet101d 78.8 44.5 224 resnet152d 79.5 60.2 224"},{"location":"metrics/historical/#efficientnet-family","title":"EfficientNet Family","text":"<p>The original compound scaling models that achieved state-of-the-art efficiency.</p>"},{"location":"metrics/historical/#efficientnet-b-series","title":"EfficientNet-B Series","text":"Model Top-1 Top-5 Params (M) Img Size efficientnet_b0 77.7 93.5 5.3 224 efficientnet_b1 79.2 94.5 7.8 240 efficientnet_b2 80.3 95.1 9.1 260 efficientnet_b3 81.7 95.9 12.2 300 efficientnet_b4 83.0 96.4 19.3 380 efficientnet_b5 83.8 96.8 30.4 456 efficientnet_b6 84.1 96.9 43.0 528 efficientnet_b7 84.4 97.1 66.3 600"},{"location":"metrics/historical/#efficientnet-v2","title":"EfficientNet-V2","text":"Model Top-1 Params (M) Img Size efficientnetv2_s 83.9 21.5 384 efficientnetv2_m 85.1 54.1 480 efficientnetv2_l 85.7 119.5 480 efficientnetv2_xl 86.2 208.1 512"},{"location":"metrics/historical/#mobilenet-family","title":"MobileNet Family","text":"<p>Efficient architectures designed for mobile and edge devices.</p>"},{"location":"metrics/historical/#mobilenet-v2","title":"MobileNet-V2","text":"Model Top-1 Params (M) Img Size mobilenetv2_050 65.4 2.0 224 mobilenetv2_100 72.0 3.5 224 mobilenetv2_140 75.0 6.1 224"},{"location":"metrics/historical/#mobilenet-v3","title":"MobileNet-V3","text":"Model Top-1 Params (M) Img Size mobilenetv3_small_050 57.9 1.5 224 mobilenetv3_small_075 65.7 2.0 224 mobilenetv3_small_100 67.7 2.5 224 mobilenetv3_large_075 73.4 4.0 224 mobilenetv3_large_100 75.2 5.5 224"},{"location":"metrics/historical/#deit-data-efficient-image-transformers","title":"DeiT (Data-efficient Image Transformers)","text":"<p>Knowledge distillation for Vision Transformers.</p> Model Top-1 Params (M) Img Size deit_tiny_patch16_224 72.2 5.7 224 deit_small_patch16_224 79.8 22.1 224 deit_base_patch16_224 81.8 86.6 224 deit_base_patch16_384 83.1 86.6 384"},{"location":"metrics/historical/#swin-transformer","title":"Swin Transformer","text":"<p>Hierarchical Vision Transformer with shifted windows.</p>"},{"location":"metrics/historical/#swin-v1","title":"Swin-V1","text":"Model Top-1 Params (M) Img Size swin_tiny_patch4_window7_224 81.2 28.3 224 swin_small_patch4_window7_224 83.2 49.6 224 swin_base_patch4_window7_224 83.5 87.8 224 swin_base_patch4_window12_384 84.5 87.9 384 swin_large_patch4_window7_224 86.3 196.5 224 swin_large_patch4_window12_384 87.3 196.7 384"},{"location":"metrics/historical/#swin-v2","title":"Swin-V2","text":"Model Top-1 Params (M) Img Size swinv2_tiny_window8_256 81.8 28.3 256 swinv2_small_window8_256 83.7 49.7 256 swinv2_base_window8_256 84.2 87.9 256 swinv2_base_window16_256 84.6 87.9 256"},{"location":"metrics/historical/#convnext","title":"ConvNeXt","text":"<p>Modern pure ConvNet architecture inspired by transformers.</p> Model Top-1 Params (M) Img Size convnext_tiny 82.1 28.6 224 convnext_small 83.1 50.2 224 convnext_base 83.8 88.6 224 convnext_large 84.3 197.8 224 convnext_xlarge 84.6 350.2 224"},{"location":"metrics/historical/#beit-bert-pre-training-for-image-transformers","title":"BEiT (BERT Pre-Training for Image Transformers)","text":"<p>Self-supervised pre-trained Vision Transformers.</p> Model Top-1 Params (M) Img Size beit_base_patch16_224 85.2 86.5 224 beit_base_patch16_384 86.8 86.7 384 beit_large_patch16_224 87.5 304.4 224 beit_large_patch16_384 88.6 305.0 384"},{"location":"metrics/historical/#regnet","title":"RegNet","text":"<p>Fast and efficient networks from Facebook AI Research.</p> Model Top-1 Params (M) Img Size regnetx_002 68.8 2.7 224 regnetx_004 72.4 5.2 224 regnetx_008 75.5 7.3 224 regnetx_016 77.0 9.2 224 regnetx_032 78.4 15.3 224 regnety_002 70.3 3.2 224 regnety_004 74.0 4.3 224 regnety_008 76.3 6.3 224 regnety_016 77.9 11.2 224 regnety_032 79.5 19.4 224"},{"location":"metrics/historical/#nfnet-normalizer-free-networks","title":"NFNet (Normalizer-Free Networks)","text":"<p>High-performance networks without batch normalization.</p> Model Top-1 Params (M) Img Size nfnet_f0 83.6 71.5 256 nfnet_f1 84.7 132.6 320 nfnet_f2 85.1 193.8 352 nfnet_f3 85.7 254.9 416 nfnet_f4 85.9 316.1 512"},{"location":"metrics/historical/#notable-training-techniques","title":"Notable Training Techniques","text":""},{"location":"metrics/historical/#augmentation-strategies","title":"Augmentation Strategies","text":"<ul> <li>AutoAugment: Learned augmentation policies</li> <li>RandAugment: Simplified random augmentation</li> <li>TrivialAugment: Minimalist augmentation</li> <li>CutMix: Cutting and mixing training images</li> <li>Mixup: Linearly interpolating images and labels</li> </ul>"},{"location":"metrics/historical/#regularization-methods","title":"Regularization Methods","text":"<ul> <li>DropPath: Stochastic depth</li> <li>DropBlock: Structured dropout</li> <li>MixUp: Label smoothing via mixing</li> <li>CutMix: Spatial mixing</li> </ul>"},{"location":"metrics/historical/#training-recipes","title":"Training Recipes","text":"<ul> <li>A3: Advanced augmentation recipe from ResNet-RS</li> <li>RA: RandAugment-based training</li> <li>DeiT: Knowledge distillation from teacher models</li> </ul>"},{"location":"metrics/historical/#legacy-but-still-relevant","title":"Legacy But Still Relevant","text":"<p>Many of these \"historical\" models remain highly relevant:</p> <ul> <li>ResNet-50 is still the standard baseline for many tasks</li> <li>EfficientNet models offer excellent efficiency for production</li> <li>MobileNet variants are widely deployed on mobile devices</li> <li>ViT-Base remains a common transformer baseline</li> </ul>"},{"location":"metrics/historical/#evolution-timeline","title":"Evolution Timeline","text":"<pre><code>2015 \u2500\u2500\u2500\u2500 ResNet\n2017 \u2500\u2500\u2500\u2500 MobileNet-V1, SENet\n2018 \u2500\u2500\u2500\u2500 MobileNet-V2, EfficientNet\n2019 \u2500\u2500\u2500\u2500 EfficientNet scaling, MobileNet-V3\n2020 \u2500\u2500\u2500\u2500 Vision Transformer (ViT), DeiT\n2021 \u2500\u2500\u2500\u2500 Swin, BEiT, ConvNeXt\n2022 \u2500\u2500\u2500\u2500 ConvNeXt-V2, EfficientNet-V2\n2023 \u2500\u2500\u2500\u2500 Modern training recipes, SBB baselines\n2024 \u2500\u2500\u2500\u2500 MobileNet-V4, MambaOut, improved recipes\n2025 \u2500\u2500\u2500\u2500 ROPE-ViT, NaFlexViT, SO150M variants\n</code></pre>"},{"location":"metrics/historical/#links","title":"Links","text":"<ul> <li>Back to Overview</li> <li>2025 Models</li> <li>2024 Models</li> </ul>"},{"location":"metrics/overview/","title":"Model Metrics Overview","text":"<p>This section contains comprehensive performance metrics for all models available in <code>timm</code>. Metrics are organized by release date for easier navigation.</p> <p>\ud83d\udd25 Complete Benchmark Table</p> <p>New! View all model benchmarks in one comprehensive sortable table:</p> <p>\u2192 Complete Benchmark Table with ALL Models</p> <p>This table includes 150+ model variants with full performance metrics, making it easy to compare models and find the best option for your use case.</p>"},{"location":"metrics/overview/#how-to-use-these-metrics","title":"How to Use These Metrics","text":"<p>All metrics shown are ImageNet-1k validation results unless otherwise specified. Key metrics include:</p> <ul> <li>top1: Top-1 accuracy (%)</li> <li>top5: Top-5 accuracy (%)</li> <li>param_count: Number of parameters (millions)</li> <li>img_size: Input image resolution</li> </ul>"},{"location":"metrics/overview/#browse-by-year","title":"Browse by Year","text":"<ul> <li>2025 Models - Latest model releases and updates</li> <li>2024 Models - Models released throughout 2024</li> <li>Historical Models - Earlier releases and classic architectures</li> </ul>"},{"location":"metrics/overview/#top-performing-models-2025","title":"Top Performing Models (2025)","text":""},{"location":"metrics/overview/#vision-transformers-vit","title":"Vision Transformers (ViT)","text":"Model Top-1 Top-5 Params (M) Image Size vit_so400m_patch14_siglip_378.webli_ft_in1k 89.42 - 400+ 378 vit_mediumd_patch16_reg4_gap_384.sbb2_e200_in12k_ft_in1k 87.438 98.256 64.11 384 vit_so150m2_patch16_reg1_gap_448.sbb_e200_in12k_ft_in1k 88.1 - 150+ 448 vit_so150m2_patch16_reg1_gap_384.sbb_e200_in12k_ft_in1k 87.9 - 150+ 384"},{"location":"metrics/overview/#mambaout-models","title":"MambaOut Models","text":"Model Top-1 Top-5 Params (M) Image Size mambaout_base_plus_rw.sw_e150_r384_in12k_ft_in1k 87.506 98.428 101.66 384 mambaout_base_plus_rw.sw_e150_in12k_ft_in1k 86.912 98.236 101.66 288 mambaout_base_plus_rw.sw_e150_in12k_ft_in1k 86.632 98.156 101.66 224"},{"location":"metrics/overview/#mobilenet-family","title":"MobileNet Family","text":"Model Top-1 Top-5 Params (M) Image Size mobilenetv4_conv_aa_large.e230_r448_in12k_ft_in1k 84.99 97.294 32.59 544 mobilenetv4_hybrid_large.ix_e600_r384_in1k 84.356 96.892 37.76 448 mobilenetv4_conv_aa_large.e230_r384_in12k_ft_in1k 84.772 97.344 32.59 480"},{"location":"metrics/overview/#resnet-family","title":"ResNet Family","text":"Model Top-1 Top-5 Params (M) Image Size resnet50d.ra4_e3600_r224_in1k 81.838 95.922 25.58 288 resnet50d.ra4_e3600_r224_in1k 80.952 95.384 25.58 224"},{"location":"metrics/overview/#model-categories","title":"Model Categories","text":""},{"location":"metrics/overview/#by-architecture-type","title":"By Architecture Type","text":"<ul> <li>Vision Transformers (ViT): Standard and optimized transformer architectures</li> <li>Hybrid Models: Combining convolutions and attention mechanisms</li> <li>ConvNets: Pure convolutional architectures (ResNet, EfficientNet, MobileNet, etc.)</li> <li>Meta-architectures: PoolFormer, MetaFormer, etc.</li> </ul>"},{"location":"metrics/overview/#by-size-category","title":"By Size Category","text":"<ul> <li>Tiny (&lt; 10M params): Optimized for mobile and edge devices</li> <li>Small (10-50M params): Balanced accuracy and efficiency</li> <li>Medium (50-100M params): High accuracy with reasonable compute</li> <li>Large (100M+ params): Maximum accuracy for server deployment</li> </ul>"},{"location":"metrics/overview/#performance-vs-efficiency","title":"Performance vs Efficiency","text":"<p>Different models offer different trade-offs between accuracy and computational efficiency:</p> <ul> <li>Mobile/Edge: MobileNetV4, EfficientNet-B0, FastViT</li> <li>Server/Cloud: ViT-Large, EVA-02, SigLIP models</li> <li>Balanced: ResNet-50D, EfficientNet-B1-B3, ViT-Base</li> </ul>"},{"location":"metrics/overview/#training-recipes","title":"Training Recipes","text":"<p>Many models use specific training recipes:</p> <ul> <li>RA4: RandAugment with 4 augmentation magnitude</li> <li>SBB: \"Searching for Better ViT Baselines\" training recipe</li> <li>IN12K: Pre-trained on ImageNet-12k before fine-tuning on ImageNet-1k</li> <li>WebLI: Pre-trained on WebLI dataset (large-scale web images)</li> </ul>"},{"location":"metrics/overview/#links","title":"Links","text":"<ul> <li>2025 Model Metrics</li> <li>2024 Model Metrics</li> <li>Historical Metrics</li> </ul>"}]}